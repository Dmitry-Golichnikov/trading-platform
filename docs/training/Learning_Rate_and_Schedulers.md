# Learning Rate и расписания

## Назначение
Learning rate определяет шаг обновления параметров. Правильная настройка и расписания критичны для скорости сходимости, устойчивости и качества модели.

## Основные параметры
- `initial_lr`: базовый шаг (например, 1e-3 для Adam, 1e-2 для SGD).
- `warmup_steps` / `warmup_epochs`: количество шагов «разогрева» от 0 до целевого LR.
- `scheduler`: тип расписания (`cosine`, `onecycle`, `step`, `exponential`, `plateau`, `cyclical`).
- `min_lr`: минимально допустимый LR (например, 1e-6).
- `max_lr`: верхняя граница (используется в CLR/OneCycle).
- `decay_rate`/`gamma`: коэффициенты для step/exponential decay.
- `patience`, `threshold`: параметры ReduceLROnPlateau.

## Распространённые схемы
- **Warmup + Cosine Annealing**: постепенное увеличение до `initial_lr`, затем плавное снижение по косинусу.
- **OneCycle**: рост LR до `max_lr`, после чего снижение ниже начального значения; часто используется с SGD.
- **Cyclical LR**: LR колеблется между `min_lr` и `max_lr` (triangular, exp, cosine).
- **Step/Exponential**: снижение LR через фиксированные интервалы эпох.
- **ReduceLROnPlateau**: уменьшение LR при отсутствии улучшений метрики.

## Рекомендации
- Применять warmup при использовании AdamW/Transformer архитектур.
- Планировать расписания вместе с оптимизатором (например, AdamW + Cosine).
- Логировать фактические значения LR в ходе тренировки (для отладки).
- Использовать автоматический подбор (Hyperband/BOHB) на первом этапе.
