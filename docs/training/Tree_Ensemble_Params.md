# Гиперпараметры деревьев и ансамблей

## Назначение
Контролируют сложность и обобщающую способность моделей типа LightGBM, CatBoost, XGBoost. Правильная настройка влияет на скорость, переобучение и качество прогнозов.

## LightGBM
- `num_leaves`: максимальное число листьев (увеличение повышает сложность).
- `max_depth`: ограничение глубины (для контроля переобучения).
- `min_data_in_leaf`: минимум объектов в листе.
- `feature_fraction`, `bagging_fraction`, `bagging_freq`: стохастизация признаков и данных.
- `lambda_l1`, `lambda_l2`: регуляризация листьев.
- `min_gain_to_split`: минимальное улучшение для создания сплита.

## CatBoost
- `iterations`: число деревьев.
- `depth`: глубина симметричных деревьев.
- `learning_rate`: шаг бустинга.
- `l2_leaf_reg`: регуляризация листьев.
- `border_count`: количество бинов для числовых признаков.
- `bagging_temperature`: стохастический компонент обучения.

## XGBoost
- `max_depth`: глубина дерева (чаще 3–8).
- `min_child_weight`: минимум веса (суммы градиентов) в листе.
- `gamma`: минимальное улучшение для сплита.
- `subsample`, `colsample_bytree`, `colsample_bylevel`: стохастизация.
- `lambda`, `alpha`: L2/L1 регуляризация.
- `max_bin`: количество бинов (для `hist`).
- `grow_policy`: `depthwise` или `lossguide`.

## Общие рекомендации
- Контролировать переобучение через комбинацию глубины, регуляризации и стохастизации.
- Использовать раннюю остановку (`early_stopping_rounds`) и логировать метрики.
- Настраивать веса классов (`scale_pos_weight`, `class_weight`) при дисбалансе.
- Тестировать разные комбинации гиперпараметров через grid/random/Bayesian search.
