# Quick Start: –ú–æ–¥—É–ª—å –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–µ–π

–ë—ã—Å—Ç—Ä–æ–µ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ –ø–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é –º–æ–¥—É–ª—è evaluation.

## –£—Å—Ç–∞–Ω–æ–≤–∫–∞

```bash
# –û—Å–Ω–æ–≤–Ω—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ —É–∂–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã
# –î–ª—è SHAP (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ):
pip install shap

# –î–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–π (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ):
pip install plotly
```

## 5-–º–∏–Ω—É—Ç–Ω—ã–π —Å—Ç–∞—Ä—Ç

### –®–∞–≥ 1: –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏ –º–æ–¥–µ–ª–∏

```python
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.datasets import make_classification

# –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ
X, y = make_classification(
    n_samples=1000,
    n_features=20,
    n_informative=15,
    random_state=42
)

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å
model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X_train, y_train)
```

### –®–∞–≥ 2: –ë—ã—Å—Ç—Ä–∞—è –æ—Ü–µ–Ω–∫–∞

```python
from src.evaluation import MetricsCalculator

# –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
y_pred = model.predict(X_test)
y_proba = model.predict_proba(X_test)

# –í—Å–µ –º–µ—Ç—Ä–∏–∫–∏ –æ–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–æ–π
metrics = MetricsCalculator.compute_metrics(
    y_test,
    y_pred,
    task_type='classification',
    y_proba=y_proba
)

# –í—ã–≤–æ–¥–∏–º –æ—Å–Ω–æ–≤–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
print(f"Accuracy: {metrics['accuracy']:.3f}")
print(f"ROC-AUC: {metrics['roc_auc']:.3f}")
print(f"F1-score: {metrics['f1']:.3f}")
```

### –®–∞–≥ 3: –ü–æ–ª–Ω—ã–π –æ—Ç—á—ë—Ç

```python
from src.evaluation import ModelEvaluationReport

# –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç—á—ë—Ç
report = ModelEvaluationReport(
    model=model,
    task_type='classification'
)

report_data = report.generate(
    X_test=X_test,
    y_test=y_test,
    X_train=X_train,
    y_train=y_train,
    output_path='model_report.html'
)

print("‚úì –û—Ç—á—ë—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤ model_report.html")
```

## –ß–∞—Å—Ç—ã–µ —Å—Ü–µ–Ω–∞—Ä–∏–∏

### –°—Ü–µ–Ω–∞—Ä–∏–π 1: –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞–ª–∏–±—Ä–æ–≤–∫–∏

```python
from src.evaluation import CalibrationMetrics

y_proba_pos = y_proba[:, 1]  # –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–≥–æ –∫–ª–∞—Å—Å–∞

# –í—ã—á–∏—Å–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏ –∫–∞–ª–∏–±—Ä–æ–≤–∫–∏
cal_metrics = CalibrationMetrics.compute_all(y_test, y_proba_pos)

print(f"Brier Score: {cal_metrics['brier_score']:.4f}")
print(f"ECE: {cal_metrics['ece']:.4f}")

# –ï—Å–ª–∏ ECE > 0.1, –º–æ–¥–µ–ª—å –ø–ª–æ—Ö–æ –∫–∞–ª–∏–±—Ä–æ–≤–∞–Ω–∞
if cal_metrics['ece'] > 0.1:
    print("‚ö† –ú–æ–¥–µ–ª—å —Ç—Ä–µ–±—É–µ—Ç –∫–∞–ª–∏–±—Ä–æ–≤–∫–∏!")
```

### –°—Ü–µ–Ω–∞—Ä–∏–π 2: –¢–æ–ø –≤–∞–∂–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤

```python
from src.evaluation import FeatureImportanceAnalyzer

feature_names = [f'feature_{i}' for i in range(X_train.shape[1])]

analyzer = FeatureImportanceAnalyzer(
    model=model,
    X=X_test,
    y=y_test,
    feature_names=feature_names
)

# –¢–æ–ø-10 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
top_features = analyzer.get_top_features(n_top=10)
print("–¢–æ–ø-10 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤:")
for i, feat in enumerate(top_features, 1):
    print(f"  {i}. {feat}")
```

### –°—Ü–µ–Ω–∞—Ä–∏–π 3: –î–µ—Ç–µ–∫—Ü–∏—è –¥—Ä–µ–π—Ñ–∞

```python
import pandas as pd
from src.evaluation import DriftDetector

# –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ DataFrame
df_train = pd.DataFrame(X_train, columns=feature_names)
df_test = pd.DataFrame(X_test, columns=feature_names)

# –î–µ—Ç–µ–∫—Ç–æ—Ä
detector = DriftDetector(df_train, df_test)

# –ë—ã—Å—Ç—Ä–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ PSI
psi_results = detector.detect_all(methods=['psi'])['psi']

# –ü—Ä–∏–∑–Ω–∞–∫–∏ —Å –¥—Ä–µ–π—Ñ–æ–º (PSI > 0.2)
drifted = psi_results[psi_results['psi'] > 0.2]
if not drifted.empty:
    print("‚ö† –û–±–Ω–∞—Ä—É–∂–µ–Ω –¥—Ä–µ–π—Ñ –≤ –ø—Ä–∏–∑–Ω–∞–∫–∞—Ö:")
    print(drifted[['feature', 'psi', 'status']])
else:
    print("‚úì –î—Ä–µ–π—Ñ–∞ –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ")
```

## CLI: –û—Ü–µ–Ω–∫–∞ –∏–∑ —Ç–µ—Ä–º–∏–Ω–∞–ª–∞

### –ë—ã—Å—Ç—Ä–∞—è –æ—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏

```bash
# –°–æ—Ö—Ä–∞–Ω–∏—Ç–µ –º–æ–¥–µ–ª—å
python -c "
from sklearn.ensemble import RandomForestClassifier
import joblib
# ... –æ–±—É—á–µ–Ω–∏–µ ...
joblib.dump(model, 'my_model.pkl')
"

# –û—Ü–µ–Ω–∏—Ç–µ —á–µ—Ä–µ–∑ CLI
python -m src.interfaces.cli evaluate model \
    --model-path my_model.pkl \
    --test-data test_data.parquet \
    --output-dir reports/
```

### –í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤

```bash
python -m src.interfaces.cli evaluate importance \
    --model-path my_model.pkl \
    --data test_data.parquet \
    --top-n 10
```

### –î–µ—Ç–µ–∫—Ü–∏—è –¥—Ä–µ–π—Ñ–∞

```bash
python -m src.interfaces.cli evaluate drift \
    --reference-data train.parquet \
    --current-data test.parquet
```

## –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å –ø–∞–π–ø–ª–∞–π–Ω–æ–º –æ–±—É—á–µ–Ω–∏—è

```python
def train_and_evaluate(X_train, X_test, y_train, y_test):
    """–ü–æ–ª–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω –æ–±—É—á–µ–Ω–∏—è –∏ –æ—Ü–µ–Ω–∫–∏."""
    from src.evaluation import (
        MetricsCalculator,
        ModelCalibrator,
        ModelEvaluationReport
    )

    # 1. –û–±—É—á–µ–Ω–∏–µ
    model = RandomForestClassifier(random_state=42)
    model.fit(X_train, y_train)

    # 2. –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
    y_pred = model.predict(X_test)
    y_proba = model.predict_proba(X_test)

    # 3. –ú–µ—Ç—Ä–∏–∫–∏
    metrics = MetricsCalculator.compute_metrics(
        y_test, y_pred,
        task_type='classification',
        y_proba=y_proba
    )

    # 4. –ö–∞–ª–∏–±—Ä–æ–≤–∫–∞ (–µ—Å–ª–∏ –Ω—É–∂–Ω–æ)
    cal_metrics = CalibrationMetrics.compute_all(y_test, y_proba[:, 1])
    if cal_metrics['ece'] > 0.1:
        calibrator = ModelCalibrator(method='isotonic')
        calibrator.fit(y_proba[:, 1], y_test)
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–∞–ª–∏–±—Ä–∞—Ç–æ—Ä –≤–º–µ—Å—Ç–µ —Å –º–æ–¥–µ–ª—å—é

    # 5. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á—ë—Ç–∞
    report = ModelEvaluationReport(model, task_type='classification')
    report.generate(
        X_test=X_test,
        y_test=y_test,
        X_train=X_train,
        y_train=y_train,
        output_path='model_report.html'
    )

    return model, metrics

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
model, metrics = train_and_evaluate(X_train, X_test, y_train, y_test)
print(f"Model ROC-AUC: {metrics['roc_auc']:.3f}")
```

## –†–∞–±–æ—Ç–∞ —Å —Ä–µ–≥—Ä–µ—Å—Å–∏–µ–π

```python
from sklearn.ensemble import RandomForestRegressor
from sklearn.datasets import make_regression
from src.evaluation import RegressionMetrics

# –î–∞–Ω–Ω—ã–µ
X, y = make_regression(n_samples=1000, n_features=10, random_state=42)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# –ú–æ–¥–µ–ª—å
model = RandomForestRegressor(random_state=42)
model.fit(X_train, y_train)

# –û—Ü–µ–Ω–∫–∞
y_pred = model.predict(X_test)
metrics = RegressionMetrics.compute_all(y_test, y_pred, n_features=10)

print(f"RMSE: {metrics['rmse']:.3f}")
print(f"MAE: {metrics['mae']:.3f}")
print(f"R¬≤: {metrics['r2']:.3f}")
```

## –ü–æ–ª–µ–∑–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã

### –ü–∞—Ç—Ç–µ—Ä–Ω 1: –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π

```python
from src.evaluation import MetricsCalculator
import pandas as pd

models = {
    'RandomForest': RandomForestClassifier(random_state=42),
    'LogisticRegression': LogisticRegression(random_state=42),
    # ... –¥—Ä—É–≥–∏–µ –º–æ–¥–µ–ª–∏
}

results = []
for name, model in models.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    y_proba = model.predict_proba(X_test) if hasattr(model, 'predict_proba') else None

    metrics = MetricsCalculator.compute_metrics(
        y_test, y_pred,
        task_type='classification',
        y_proba=y_proba
    )

    results.append({
        'model': name,
        'accuracy': metrics['accuracy'],
        'roc_auc': metrics.get('roc_auc', None),
        'f1': metrics['f1']
    })

df_results = pd.DataFrame(results).sort_values('roc_auc', ascending=False)
print(df_results)
```

### –ü–∞—Ç—Ç–µ—Ä–Ω 2: –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –≤ –ø—Ä–æ–¥–∞–∫—à–µ–Ω–µ

```python
from src.evaluation import DriftDetector, PopulationStabilityIndex
import pandas as pd

def monitor_production_data(train_data, prod_data, threshold=0.2):
    """–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –¥—Ä–µ–π—Ñ–∞ –≤ –ø—Ä–æ–¥–∞–∫—à–µ–Ω–µ."""

    detector = DriftDetector(train_data, prod_data)

    # PSI –¥–ª—è –≤—Å–µ—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    psi_results = detector.detect_all(methods=['psi'])['psi']

    # –ê–ª–µ—Ä—Ç—ã
    critical_drift = psi_results[psi_results['psi'] > threshold]

    if not critical_drift.empty:
        print("üö® –ê–õ–ï–†–¢: –û–±–Ω–∞—Ä—É–∂–µ–Ω –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–π –¥—Ä–µ–π—Ñ!")
        print(critical_drift[['feature', 'psi', 'status']])
        return False
    else:
        print("‚úì –î–∞–Ω–Ω—ã–µ —Å—Ç–∞–±–∏–ª—å–Ω—ã")
        return True

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
is_stable = monitor_production_data(df_train, df_production, threshold=0.2)
if not is_stable:
    # –û—Ç–ø—Ä–∞–≤–∏—Ç—å –∞–ª–µ—Ä—Ç, –ø–µ—Ä–µ–æ–±—É—á–∏—Ç—å –º–æ–¥–µ–ª—å, etc.
    pass
```

### –ü–∞—Ç—Ç–µ—Ä–Ω 3: –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –∫–∞–ª–∏–±—Ä–æ–≤–∫–∞

```python
from src.evaluation import ModelCalibrator, compare_calibration_methods

def auto_calibrate(model, X_cal, y_cal):
    """–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –≤—ã–±–æ—Ä –ª—É—á—à–µ–≥–æ –º–µ—Ç–æ–¥–∞ –∫–∞–ª–∏–±—Ä–æ–≤–∫–∏."""

    # –ü–æ–ª—É—á–∞–µ–º –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏
    y_proba = model.predict_proba(X_cal)[:, 1]

    # –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º –º–µ—Ç–æ–¥—ã
    comparison = compare_calibration_methods(y_proba, y_cal)

    # –í—ã–±–∏—Ä–∞–µ–º –ª—É—á—à–∏–π –ø–æ Brier Score
    best_method = min(
        [m for m in comparison if m != 'uncalibrated'],
        key=lambda m: comparison[m].get('brier_score', float('inf'))
    )

    print(f"–õ—É—á—à–∏–π –º–µ—Ç–æ–¥ –∫–∞–ª–∏–±—Ä–æ–≤–∫–∏: {best_method}")

    # –û–±—É—á–∞–µ–º –ª—É—á—à–∏–π –∫–∞–ª–∏–±—Ä–∞—Ç–æ—Ä
    calibrator = ModelCalibrator(method=best_method)
    calibrator.fit(y_proba, y_cal)

    return calibrator

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
calibrator = auto_calibrate(model, X_test, y_test)
```

## –°–æ–≤–µ—Ç—ã –∏ —Ç—Ä—é–∫–∏

### 1. –≠–∫–æ–Ω–æ–º–∏—è –ø–∞–º—è—Ç–∏ –¥–ª—è –±–æ–ª—å—à–∏—Ö –¥–∞–Ω–Ω—ã—Ö

```python
# –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –ø–æ–¥–≤—ã–±–æ—Ä–∫—É –¥–ª—è SHAP
from src.evaluation import SHAPImportance

shap_df = SHAPImportance.compute(
    model, X_test,
    max_samples=100  # –¢–æ–ª—å–∫–æ 100 —Å—ç–º–ø–ª–æ–≤
)
```

### 2. –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫

```python
from joblib import Parallel, delayed

def compute_metrics_parallel(models, X_test, y_test):
    def evaluate_model(name, model):
        y_pred = model.predict(X_test)
        metrics = MetricsCalculator.compute_metrics(y_test, y_pred)
        return name, metrics

    results = Parallel(n_jobs=-1)(
        delayed(evaluate_model)(name, model)
        for name, model in models.items()
    )

    return dict(results)
```

### 3. –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è

```python
from functools import lru_cache
import hashlib
import pickle

@lru_cache(maxsize=32)
def cached_drift_detection(train_hash, test_hash):
    # –î–µ—Ç–µ–∫—Ü–∏—è –¥—Ä–µ–π—Ñ–∞ —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º
    pass
```

## –ß—Ç–æ –¥–∞–ª—å—à–µ?

- üìñ [–ü–æ–ª–Ω–∞—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è](README.md)
- üìä [–ü—Ä–∏–º–µ—Ä—ã –æ—Ç—á—ë—Ç–æ–≤](../../examples/evaluation/)
- üß™ [–Æ–Ω–∏—Ç-—Ç–µ—Å—Ç—ã](../../tests/unit/test_evaluation_*.py)
- üîß [–†–∞—Å—à–∏—Ä–µ–Ω–∏–µ –º–æ–¥—É–ª—è](README.md#—Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ-–º–æ–¥—É–ª—è)

## –ü–æ–¥–¥–µ—Ä–∂–∫–∞

–í–æ–ø—Ä–æ—Å—ã? –°–æ–∑–¥–∞–π—Ç–µ issue –∏–ª–∏ –ø–æ—Å–º–æ—Ç—Ä–∏—Ç–µ FAQ –≤ –æ—Å–Ω–æ–≤–Ω–æ–π –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏.
