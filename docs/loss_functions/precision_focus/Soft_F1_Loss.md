# Soft F1 Loss

## Назначение
Дифференцируемая аппроксимация F1-score для обучения моделей, ориентированных на баланс Precision и Recall. Уменьшает штраф за ложные сигналы.

## Формула (пример)
\( \text{SoftF1} = 1 - \frac{2 \sum (\hat{y} y)}{\sum \hat{y} + \sum y} \) с небольшими \(\epsilon\) для численной стабильности. Потеря = SoftF1.

## Особенности
- Согласует обучение с целевой метрикой F1, сохраняя дифференцируемость.
- Выравнивает влияние Precision и Recall.
- Может быть нестабильна при очень малом количестве положительных примеров (нужны регуляризация и сглаживание).

## Рекомендации
- Использовать для задач, где важно повышать Precision без сильного падения Recall.
- Комбинировать с BCE или Focal Loss (multi-task), чтобы сохранить уверенность прогноза.
