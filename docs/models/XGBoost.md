# XGBoost

## Назначение
Оптимизированный градиентный бустинг на деревьях для решения задач классификации и регрессии. Часто используется как сильная табличная модель для прогнозов long/short.

## Ключевые особенности
- Высокая производительность и гибкость настройки регуляризации.
- Поддержка взвешивания классов, кастомных функций потерь и метрик.
- GPU-ускорение (tree_method=`gpu_hist`) и распределённое обучение.

## Типичные гиперпараметры
- `eta` (learning_rate), `max_depth`, `min_child_weight` — контролируют сложность деревьев.
- `subsample`, `colsample_bytree`, `colsample_bylevel` — стохастизация.
- `gamma`, `lambda`, `alpha` — регуляризация (минимальная потеря при сплите, L2, L1).
- `objective`: `binary:logistic`, `multi:softprob`, пользовательская функция для long-only/short-only.

## Требования к данным
- Признаки желательно масштабировать/нормализовать, если есть разные порядки величин.
- Обработка пропусков может выполняться автоматически (встроенные направления ветвлений).
- Для однонаправленных сценариев подготовить таргеты (например, только long против flat).

## Метрики и валидация
- Стандартные: Accuracy, F1, ROC-AUC, PR-AUC.
- Трейдинговые: Sharpe, Hit Rate, Profit Factor (кастомные метрики через callback).
- Walk-forward/expanding window; поддержка встроенной кросс-валидации (`xgb.cv`).

## Интеграция в конвейер
- Использовать `XGBClassifier`/`XGBRegressor` или низкоуровневый API.
- Сохранять модели в бинарном формате (`.json`/`.model`) для повторного использования.
- Вести лог важности признаков (gain, weight, cover) и SHAP-анализ.

## Ограничения и рекомендации
- Возможны проблемы с переобучением при малом объёме данных; использовать раннюю остановку.
- Требует аккуратного подбора гиперпараметров, особенно для режимов long/short.
- Меньше отвечает требованиям интерпретируемости, чем линейные модели; дополнять SHAP/Permutation Importance.
