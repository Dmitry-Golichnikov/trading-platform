# GRU (Gated Recurrent Unit)

## Назначение
Рекуррентная нейросеть с облегчённой структурой по сравнению с LSTM. Используется для прогнозирования направлений long/short в задачах с последовательными данными.

## Ключевые особенности
- Меньше параметров, чем LSTM, быстрее обучается.
- Улавливает временные зависимости через механизмы обновления и сброса.
- Подходит для режимов long-only, short-only и мультиклассовых задач.

## Типичные гиперпараметры
- `hidden_size`, `num_layers`, `dropout`, `bidirectional`.
- `batch_size`, `sequence_length`, `learning_rate`, `optimizer`.
- Gradient clipping для стабилизации обучения.

## Требования к данным
- Формирование последовательностей с фиксацией длины окна.
- Нормализация признаков в каждом окне (z-score, min-max).
- Обучение на отфильтрованных таргетах в зависимости от выбранного направления.

## Метрики и валидация
- Accuracy, F1, ROC-AUC, трейдинговые показатели (Sharpe, Hit Rate).
- Walk-forward/expanding window; ранняя остановка.
- Анализ скрытых состояний и attention (если используется).

## Интеграция в конвейер
- PyTorch-реализация с унифицированным интерфейсом.
- Сохранение весов, параметров оптимизатора и нормировочных статистик.
- Возможна комбинация с CNN/attention слоями для улучшения качества.

## Ограничения и рекомендации
- Может недовоспроизводить длинные зависимости по сравнению с LSTM/Transformer.
- Требует подбора длины окна и числа слоёв.
- Рекомендуется использовать как более лёгкую альтернативу LSTM для быстрого прототипирования.
