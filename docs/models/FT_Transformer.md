# FT-Transformer

## Назначение
Трансформер для табличных данных, использующий attention-механизм для обработки признаков. Подходит для комплексных нелинейных зависимостей и прогнозов long/short на богатых наборах индикаторов.

## Ключевые особенности
- Моделирует взаимодействия между признаками через multi-head attention.
- Поддерживает работу с числовыми и категориальными данными.
- Эффективно обучается на GPU, реализуется в PyTorch.

## Типичные гиперпараметры
- `n_layers`, `n_heads`, `d_model`, `d_ff`: архитектура трансформера.
- `dropout`, `attention_dropout`: регуляризация.
- `batch_size`, `learning_rate`, `optimizer`, `weight_decay`.
- Настройки head-ов для разных направлений (long-only, short-only, multi-class).

## Требования к данным
- Стандартизация числовых признаков, embedding для категориальных.
- Обработка пропусков через маски или заполнение.
- Формирование таргетов и выбор режимов long/short.

## Метрики и валидация
- Accuracy, F1, ROC-AUC и трейдинговые метрики (Sharpe, Profit Factor).
- Логирование attention-матриц для интерпретации, мониторинг валидационной ошибки.
- Walk-forward/expanding window.

## Интеграция в конвейер
- Реализация в PyTorch, соблюдение интерфейса `fit/predict/predict_proba`.
- Хранить чекпоинты, конфигурацию архитектуры и нормировочные параметры.
- Использовать mixed precision для ускорения на GPU.

## Ограничения и рекомендации
- Требует значительных вычислительных ресурсов и аккуратной настройки.
- Подвержен переобучению на малых данных — использовать регуляризацию и data augmentation.
- Рекомендуется иметь baseline (LightGBM, CatBoost) для сравнения.
