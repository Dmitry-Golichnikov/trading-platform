# TabNet

## Назначение
Нейросетевая архитектура для табличных данных, использующая последовательные шаги внимания по признакам. Подходит для выявления сложных нелинейных зависимостей между индикаторами и прогнозирования сигналов long/short.

## Ключевые особенности
- Отбор признаков через маски внимания на каждом шаге, интерпретируемость важности.
- Эффективное обучение на GPU, оптимизировано для табличных данных.
- Поддержка режимов long-only, short-only и многоклассовой классификации.

## Типичные гиперпараметры
- `n_d`, `n_a`: размерность блоков решений и внимания.
- `n_steps`: количество шагов внимания.
- `gamma`: коэффициент разреженности масок.
- `lambda_sparse`: регуляризация разреженности.
- `optimizer`, `learning_rate`, `batch_size`.

## Требования к данным
- Предпочтительно масштабирование признаков.
- Обработка пропусков через дополнительные индикаторы или заполнение.
- Формирование таргетов в зависимости от направления (long-only/short-only/long+short).

## Метрики и валидация
- Классификационные (Accuracy, F1, ROC-AUC) и трейдинговые показатели.
- Слежение за loss на валидации с ранней остановкой.
- Walk-forward/expanding window для временных рядов.

## Интеграция в конвейер
- Реализация на PyTorch с унифицированным интерфейсом.
- Логирование масок внимания и важности признаков для анализа.
- Сохранение весов модели и параметров оптимизатора.

## Ограничения и рекомендации
- Требует тщательной настройки гиперпараметров и регуляризации.
- Чувствителен к шумным и сильно коррелированным признакам (использовать фильтрацию).
- Потребляет больше ресурсов по сравнению с градиентным бустингом; проверять на baseline.
