# Informer

## Назначение
Эффективный трансформер для длинных временных рядов с механизмом ProbSparse attention. Подходит для прогнозирования направленных сигналов long/short на больших временных окнах и множестве индикаторов.

## Ключевые особенности
- ProbSparse attention сокращает вычислительную сложность.
- Декодер с генерацией прогнозов на несколько шагов вперёд.
- Модифицированная архитектура трансформера со свёрточными distilling-слоями.

## Типичные гиперпараметры
- `d_model`, `n_heads`, `e_layers`, `d_layers`, `d_ff`.
- `factor`: управляет ProbSparse attention.
- `seq_len`, `label_len`, `pred_len`, `dropout`, `learning_rate`.
- `optimizer`, `batch_size`, `activation`.

## Требования к данным
- Подготовка входных последовательностей (encoder input, decoder input, target).
- Нормализация, маскирование пропусков, выделение категориальных признаков.
- Настройка таргетов на long-only, short-only или мультикласс.

## Метрики и валидация
- Accuracy, F1, ROC-AUC; MSE/MAE если используется регрессионный выход.
- Трейдинговые метрики (Sharpe, Profit Factor).
- Walk-forward/expanding window; мониторинг loss и metрик на валидации.

## Интеграция в конвейер
- Реализация на PyTorch, интеграция через унифицированный интерфейс.
- Логирование attention карт, loss, трейдинговых метрик.
- Сохранять параметры модели, статистику нормализации, конфигурацию длины окон.

## Ограничения и рекомендации
- Высокие требования к подготовке данных и гиперпараметрам.
- При небольших выборках может переобучаться; использовать регуляризацию и early stopping.
- Сравнивать с более простыми моделями (LSTM, TFT) для оценки эффективности.
