# Temporal Fusion Transformer (TFT)

## Назначение
Архитектура трансформера для временных рядов с гибридными компонентами (LSTM + attention + gating). Предназначена для прогнозирования направлений long/short на множестве временных признаков, категориальных и непрерывных.

## Ключевые особенности
- Комбинация рекуррентных блоков и self-attention, что позволяет моделировать локальные и глобальные зависимости.
- Variable Selection Network автоматически выделяет важные признаки на каждом временном шаге.
- Интерпретируемость через attention weights и importance score.

## Типичные гиперпараметры
- `hidden_size`, `num_heads`, `dropout`, `embedding_dim`.
- `stack_size`, `num_lstm_layers`, `attention_dropout`.
- `batch_size`, `learning_rate`, `max_encoder_length`, `max_prediction_length`.

## Требования к данным
- Разделение признаков на известные заранее, неизвестные, статические категории.
- Нормализация и обработка пропусков.
- Настройка таргетов для long-only, short-only или мультиклассового прогноза.

## Метрики и валидация
- Классификационные (Accuracy, F1) и трейдинговые (Sharpe, Hit Rate, Profit Factor).
- Мониторинг валидационной потери, ранняя остановка.
- Walk-forward/expanding window валидация.

## Интеграция в конвейер
- Реализация на PyTorch Lightning или кастомном PyTorch с унифицированным интерфейсом.
- Логирование attention weights, variable selection weights, прогнозов.
- Сохранять чекпоинты и конфигурацию архитектуры (encoder/decoder длины).

## Ограничения и рекомендации
- Требует значительных вычислительных ресурсов и аккуратного тюнинга.
- Чувствителен к качеству подготовки признаков и категориальных embeddings.
- Рекомендуется иметь baseline (LSTM, LightGBM) для оценки целесообразности.
