# Seq2Seq с механизмом внимания

## Назначение
Архитектура encoder-decoder для прогнозирования последовательностей, дополненная attention. Используется для моделирования сложных временных паттернов и прогнозирования направлений long/short на горизонте нескольких шагов.

## Ключевые особенности
- Encoder агрегирует входные данные, decoder генерирует последовательность прогнозов.
- Механизм внимания выделяет важные временные шаги и признаки.
- Подходит для прогнозов нескольких горизонтов (multi-step ahead) и режимов long-only/short-only.

## Типичные гиперпараметры
- Тип рекуррентных блоков (LSTM, GRU), `hidden_size`, `num_layers`.
- Размер attention (Bahdanau, Luong), dropout, teacher forcing ratio.
- `learning_rate`, `optimizer`, `batch_size`, `sequence_length`.

## Требования к данным
- Подготовка входных/выходных последовательностей (encoder input, decoder target).
- Нормализация признаков и маскирование пропусков.
- Разметка таргетов для выбранного направления торговли.

## Метрики и валидация
- Классификационные метрики (Accuracy, F1) для каждого горизонта.
- Трейдинговые метрики на агрегированном прогнозе (Sharpe, Profit Factor).
- Walk-forward/expanding window, мониторинг teacher forcing/validation loss.

## Интеграция в конвейер
- Реализуется в PyTorch, предоставляет интерфейс `fit/predict` с генерацией последовательностей.
- Логирование attention heatmap для интерпретации.
- Сохранять веса encoder/decoder, параметры обучения и статистику нормализации.

## Ограничения и рекомендации
- Сложность обучения и риск переобучения; использовать регуляризацию и dropout.
- Большие требования к вычислительным ресурсам.
- Рекомендуется сравнивать с упрощёнными моделями (LSTM, GRU) для оценки прироста.
