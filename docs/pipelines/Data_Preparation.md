# Подготовка данных

## Назначение
Обеспечить чистый, синхронизированный и готовый к feature engineering набор данных. На этом этапе устраняются пропуски, ошибки и несогласованности между источниками.

## Основные шаги
1. **Загрузка и проверка целостности**
   - Поддерживаемые источники: CSV/Parquet, брокерские API, маркет-дат провайдеры.
   - Проверка: наличие всех колонок, корректность временных меток, отсутствие NaN/inf.
2. **Очистка и фильтрация**
   - Удаление аномалий (шипы, dupe ticks, неверные цены/объёмы).
   - Фильтры ликвидности (минимальный объём/количество сделок).
   - Исключение технических пауз/отключений.
3. **Синхронизация таймзон и календаря**
   - Приведение к единой временной зоне (UTC).
   - Выравнивание праздников/нерабочих дней, добавление флагов отсутствия торгов.
4. **Resample и согласование частот**
   - Преобразование тиковых данных в бары (OHLCV) заданного таймфрейма.
   - Синхронизация нескольких источников (asof join, forward-fill).
5. **Базовые фильтры**
   - Удаление пропусков/интерполяция (forward-fill c ограничением).
   - Маскировка опасных периодов (новости, roll-over) по заданному календарю.

## Артефакты и логирование
- Логи очистки (сколько записей удалено, какие фильтры сработали).
- Сохранение очищенных данных в `data/clean/` или Data Lake (Parquet).
- Метаданные: диапазон дат, список тикеров, версия фильтров.

## Рекомендации
- Каждое преобразование делать идемпотентным и конфигурируемым (YAML/JSON).
- Хранить сырые данные отдельно для повторной обработки.
- Вести версии подготовленных датасетов (dataset_id, commit hash).
