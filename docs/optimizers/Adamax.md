# Adamax

## Назначение
Вариант Adam, использующий бесконечную норму (L∞) для оценки масштаба градиента. Стабильный при градиентах с большой амплитудой.

## Особенности
- Обновление второго момента заменяется максимумом по модулю градиента.
- Параметры: `learning_rate`, `betas`, `eps`.
- Подходит для задач с редкими, но сильными скачками градиента.

## Рекомендации
- Применять, когда стандартный Adam нестабилен из-за крупных градиентов.
- В PyTorch доступен как `torch.optim.Adamax`.
