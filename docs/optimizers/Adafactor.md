# Adafactor

## Назначение
Адаптивный оптимизатор с низким использованием памяти. Идеален для крупных трансформеров (T5) и seq2seq моделей.

## Особенности
- Сохраняет факторизованные матрицы моментов (`row` и `column`) вместо полного тензора.
- Автоматически масштабирует learning rate в зависимости от параметров.
- Может работать без явного `learning_rate` (адаптивный шаг).

## Рекомендации
- Использовать в режиме mixed precision и на больших batch.
- Комбинировать с warmup и линейным/косинусным decay.
- Для небольших моделей можно предпочесть AdamW.
