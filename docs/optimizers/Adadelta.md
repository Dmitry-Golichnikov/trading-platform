# Adadelta

## Назначение
Улучшение Adagrad с ограничением накопления градиентов. Сохраняет адаптивность шага, но предотвращает слишком быстрое уменьшение learning rate.

## Принцип работы
- Использует экспоненциальное скользящее среднее квадратов градиентов (`E[g^2]_t`).
- Обновления масштабируются отношением накопленных квадратичных обновлений и градиентов.

## Гиперпараметры
- `rho` (ρ): коэффициент EMA (обычно 0.9).
- `eps`: стабилизатор (1e-6).
- `learning_rate`: по умолчанию 1.0 (можно не задавать).

## Особенности
- Хорошо работает при разреженных признаках.
- Менее чувствителен к выбору начального learning rate.
- Подходит для задач, где сложно подобрать шаг вручную.

## Рекомендации
- Применять в сочетании с gradient clipping для стабильности.
- При необходимости более агрессивного обучения переходить на Adam/AdamW после начальной фазы.
