# AdamW

## Назначение
Модификация Adam с корректной реализацией weight decay (L2-регуляризации). Стандарт де-факто для современных трансформеров и глубоких сетей.

## Особенности
- Weight decay применяется отдельно от шага градиента, что улучшает обобщающую способность.
- Поддерживает все параметры Adam (`learning_rate`, `betas`, `eps`) плюс `weight_decay`.
- Хорошо сочетается с warmup и Cosine Annealing.

## Рекомендации
- Использовать для моделей на PyTorch (`torch.optim.AdamW`).
- Настроить linear/cosine warmup на 5–10% эпох.
- Включать gradient clipping, особенно на последовательных моделях.
