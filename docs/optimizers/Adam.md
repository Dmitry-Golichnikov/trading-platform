# Adam (Adaptive Moment Estimation)

## Назначение
Популярный адаптивный оптимизатор, комбинирующий Momentum и RMSProp. Подходит для большинства нейросетей, включая трансформеры и рекуррентные модели.

## Формулы
- Поддерживаются первые и вторые моменты градиентов: `m_t`, `v_t`.
- Обновление: `θ_{t+1} = θ_t - η * (m̂_t / (√(v̂_t) + ε))`, где `m̂_t`, `v̂_t` — смещённые оценки.

## Гиперпараметры
- `learning_rate` (η): базовый шаг (1e-3 по умолчанию).
- `betas`: `(β1, β2)` — коэфф. экспоненциального среднего (часто 0.9 и 0.999).
- `eps`: стабилизатор (1e-8).
- `weight_decay`: L2-регуляризация (по умолчанию 0).

## Особенности
- Быстро сходится, устойчив к масштабированию градиентов.
- Может приводить к переобучению — использовать weight decay или перейти на AdamW.
- Чувствителен к выбору learning rate — рекомендован scheduler и warmup.

## Рекомендации
- Для трансформеров использовать warmup + decay (Cosine, Polynomial).
- Комбинировать с gradient clipping и EMA.
- При дисбалансе классов настройка происходит через loss, оптимизатор не требует изменений.
