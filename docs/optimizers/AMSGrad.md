# AMSGrad

## Назначение
Вариант Adam с гарантией сходимости. Использует максимум оценок второго момента для стабилизации шага.

## Особенности
- Поддерживает те же параметры, что Adam (`learning_rate`, `betas`, `eps`).
- Обновление второго момента: `v̂_t = max(v̂_{t-1}, v_t)`.
- Предотвращает проблемы с увеличением шага при уменьшении `v_t`.

## Рекомендации
- Применять, если стандартный Adam демонстрирует нестабильность или расходимость.
- В PyTorch включается флагом `amsgrad=True` в Adam/AdamW.
- Сохранять те же расписания learning rate, что и для Adam.
