# Adagrad

## Назначение
Адаптивный оптимизатор, автоматически уменьшающий скорость обучения для часто обновляемых параметров. Полезен при работе с разреженными признаками и в начальной фазе обучения.

## Формула обновления
- Поддерживается накопленная сумма квадратов градиентов `G_t`.
- Обновление: `θ_{t+1} = θ_t - (η / (√(G_t) + ε)) ⊙ ∇L(θ_t)`.

## Гиперпараметры
- `learning_rate` (η): базовый шаг (по умолчанию > 0.01).
- `eps` (ε): стабилизирующее значение (обычно 1e-8).

## Особенности
- Сильное уменьшение шага приводит к раннему затуханию обучения.
- Хорошо для задач с разреженными признаками (например, one-hot тикеры, буквенные признаки).
- Не требует сложных настроек.

## Рекомендации
- Использовать для первой стадии обучения или в задачах с большим количеством категориальных признаков.
- При затухании шага переключаться на Adadelta/Adam.
