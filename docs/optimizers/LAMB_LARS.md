# LAMB / LARS

## Назначение
- **LAMB** (Layer-wise Adaptive Moments) и **LARS** (Layer-wise Adaptive Rate Scaling) используются для обучения моделей с очень большими batch (тысячи примеров) без деградации.

## Особенности
- Регулируют скорость обучения для каждого слоя в зависимости от нормы веса и градиента.
- LAMB основан на Adam, LARS — на SGD.
- Требуют использования warmup и корректной нормализации весов.

## Рекомендации
- Применять в распределённых тренировках на многих GPU/TPU.
- Совмещать с mixed precision и линейным ростом learning rate.
- Подходят для трансформеров, CNN, когда batch size > 2048.
