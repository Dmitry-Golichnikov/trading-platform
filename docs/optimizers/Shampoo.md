# Shampoo

## Назначение
Второй порядок (quasi-second-order) оптимизатор, использующий матрицы Гессиана по блокам. Подходит для крупных трансформеров и моделей с большим количеством параметров.

## Особенности
- Рассчитывает корни матриц Гессиана по размерностям тензоров.
- Требует значительных ресурсов памяти, но обеспечивает быстрый прогресс.
- Реализации: `torch.optim.Shampoo` (DeepSpeed), `tensorflow-optimizer`.

## Рекомендации
- Использовать с preconditioning (добавление ε в диагональ).
- Применять совместно с Adafactor/AdamW fallback для маленьких параметров.
