# SGD with Momentum / Nesterov

## Назначение
Классический стохастический градиентный спуск с ускорением за счёт накопления импульса. Используется для нейросетей и линейных моделей, когда важен контроль над обновлениями и проста реализация.

## Формулы
- Momentum: `v = μ v + η ∇L(θ); θ = θ - v`
- Nesterov Momentum: градиент вычисляется в точке `θ - μ v`, что даёт лучшее предвидение траектории.

## Гиперпараметры
- `learning_rate` (η): шаг обучения.
- `momentum` (μ): коэффициент импульса (обычно 0.9).
- `weight_decay`: L2-регуляризация.
- `nesterov`: включение Nesterov-версии.

## Особенности
- Простой и эффективный на больших датасетах.
- Требует настройки LR scheduler для быстрого сходимости.
- Может «перескакивать» при шумных градиентах — применять gradient clipping.

## Рекомендации
- Использовать вместе с Cosine Annealing или StepLR.
- Для задач с дисбалансом корректировать лоссы/градиенты перед оптимизацией.
- Подходит для baseline нейросетей и легковесных моделей.
