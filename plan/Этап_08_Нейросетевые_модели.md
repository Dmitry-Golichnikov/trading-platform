# Этап 08: Нейросетевые модели для временных рядов

## Цель
Реализовать нейросетевые модели для sequence data: LSTM, GRU, TCN, Transformers.

## Зависимости
Этап 06, 07

## Модели

- **LSTM** - базовая рекуррентная
- **GRU** - упрощённая LSTM
- **Seq2Seq with Attention**
- **TCN** - Temporal Convolutional Network
- **TFT** - Temporal Fusion Transformer
- **Informer** - эффективный Transformer
- **1D-CNN+LSTM** - гибрид

## Структура

```
src/modeling/models/neural/sequential/
├── base.py                       # BaseSequentialModel
├── lstm.py
├── gru.py
├── seq2seq_attention.py
├── tcn.py
├── tft.py
├── informer.py
├── cnn_lstm.py
└── datasets.py                   # PyTorch Datasets
```

## Базовая архитектура

```python
class BaseSequentialModel(BaseModel):
    """Базовый класс для sequence моделей"""

    def __init__(
        self,
        input_size: int,
        hidden_size: int,
        num_layers: int,
        seq_length: int,
        dropout: float = 0.2,
        ...
    ):
        self.seq_length = seq_length
        # ...

    def prepare_sequences(self, X: pd.DataFrame) -> torch.Tensor:
        """Преобразовать DataFrame в sequences"""
        # Sliding window для создания последовательностей

    def fit(self, X, y, ...):
        """Training loop с PyTorch"""
        train_loader = self._create_dataloader(X, y, shuffle=True)
        val_loader = self._create_dataloader(X_val, y_val, shuffle=False)

        for epoch in range(self.epochs):
            train_loss = self._train_epoch(train_loader)
            val_loss = self._validate_epoch(val_loader)
            # Callbacks, early stopping
```

## Ключевые компоненты

### 1. Sequence preparation
```python
class SequenceDataset(Dataset):
    """PyTorch Dataset для временных рядов"""

    def __init__(
        self,
        X: np.ndarray,
        y: np.ndarray,
        seq_length: int,
        stride: int = 1
    ):
        self.X = X
        self.y = y
        self.seq_length = seq_length
        self.stride = stride

    def __getitem__(self, idx):
        start = idx * self.stride
        end = start + self.seq_length

        return {
            'X': torch.FloatTensor(self.X[start:end]),
            'y': torch.FloatTensor([self.y[end]])
        }
```

### 2. LSTM Implementation
```python
class LSTMModel(BaseSequentialModel, nn.Module):
    def __init__(self, input_size, hidden_size, num_layers, ...):
        super().__init__()
        self.lstm = nn.LSTM(
            input_size,
            hidden_size,
            num_layers,
            batch_first=True,
            dropout=dropout
        )
        self.fc = nn.Linear(hidden_size, output_size)

    def forward(self, x):
        lstm_out, _ = self.lstm(x)
        return self.fc(lstm_out[:, -1, :])
```

### 3. Training utilities
- Learning rate schedulers (OneCycle, Cosine Annealing)
- Gradient clipping
- Mixed precision training (AMP)
- Data augmentation для sequences

## Конфиг примеры

**configs/models/lstm_default.yaml:**
```yaml
model_type: lstm
task: classification

architecture:
  input_size: auto  # Определяется из данных
  hidden_size: 128
  num_layers: 2
  seq_length: 50
  dropout: 0.2
  bidirectional: false

training:
  batch_size: 128
  epochs: 100
  learning_rate: 0.001
  optimizer: adam
  scheduler: onecycle
  early_stopping: 10

device: gpu
mixed_precision: true
```

## Критерии готовности

- [ ] Все sequential модели реализованы
- [ ] Sequence preparation работает
- [ ] Training loop с callbacks
- [ ] GPU support и mixed precision
- [ ] Early stopping
- [ ] Конфиги для всех моделей
- [ ] Визуализация training curves

## Промпты

### Промпт 1: Базовые RNN модели
```
Реализуй в src/modeling/models/neural/sequential/:

1. base.py - BaseSequentialModel с:
   - Sequence preparation
   - Training loop (PyTorch)
   - Callbacks integration
   - Device management

2. lstm.py - LSTM модель (uni/bidirectional)
3. gru.py - GRU модель

Используй PyTorch Lightning для упрощения.
```

### Промпт 2: Advanced архитектуры
```
Реализуй:
1. seq2seq_attention.py - Seq2Seq с attention mechanism
2. tcn.py - Temporal Convolutional Network
3. tft.py - Temporal Fusion Transformer
4. cnn_lstm.py - Hybrid CNN+LSTM

Для TFT используй готовую имплементацию (pytorch-forecasting).
```

### Промпт 3: Training utilities
```
Создай:
1. datasets.py - SequenceDataset, DataLoaders
2. augmentation.py - Augmentation для sequences (jitter, scaling, etc)
3. schedulers.py - LR schedulers обёртки

Добавь visualization.py для:
- Training curves
- Attention weights visualizations
- Predictions vs ground truth
```

## Важные замечания

**Последовательности:**
- Длина sequence (seq_length) - гиперпараметр
- Stride для sliding window
- Padding для коротких последовательностей

**Производительность:**
- Batch processing обязательно
- Mixed precision (AMP) для больших моделей
- DataLoader с num_workers > 0

**Overfitting:**
- Dropout
- Sequence-aware augmentation
- Early stopping на validation

**Look-ahead:**
Убедиться что sequences создаются без look-ahead bias!

## Следующий этап
[Этап 09: Система обучения и оптимизации](Этап_09_Система_обучения.md)
