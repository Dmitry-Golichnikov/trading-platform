# Этап 07: Классические ML модели

## Цель
Реализовать обёртки для классических ML моделей: Linear, Tree-based, Tabular Neural Networks.

## Зависимости
Этап 06

## Модели для реализации

### Tree-based:
- **LightGBM** (приоритет #1)
- **XGBoost**
- **CatBoost**
- **RandomForest**
- **ExtraTrees**

### Linear:
- **LogisticRegression**
- **ElasticNet**

### Tabular NN:
- **TabNet**
- **FT-Transformer**
- **NODE**

## Структура реализации

```python
@ModelRegistry.register('lightgbm')
class LightGBMModel(BaseModel):
    """LightGBM обёртка"""

    def __init__(self, task='classification', **hyperparams):
        self.task = task
        self.hyperparams = hyperparams
        self.model = None

    def fit(self, X, y, X_val=None, y_val=None, **kwargs):
        """Обучение с early stopping на validation"""
        import lightgbm as lgb

        train_data = lgb.Dataset(X, y)
        valid_data = lgb.Dataset(X_val, y_val) if X_val is not None else None

        self.model = lgb.train(
            self.hyperparams,
            train_data,
            valid_sets=[valid_data] if valid_data else None,
            callbacks=[lgb.early_stopping(50), lgb.log_evaluation(10)]
        )
        return self

    @property
    def feature_importances_(self):
        return self.model.feature_importance(importance_type='gain')
```

## Hyperparameters (default configs)

**configs/models/lightgbm_default.yaml:**
```yaml
model_type: lightgbm
task: classification

hyperparameters:
  objective: binary
  metric: auc
  boosting_type: gbdt
  num_leaves: 31
  learning_rate: 0.05
  feature_fraction: 0.8
  bagging_fraction: 0.8
  bagging_freq: 5
  max_depth: -1
  min_data_in_leaf: 20
  n_estimators: 1000
  verbose: -1
  device: cpu  # или gpu
```

## Критерии готовности

- [ ] Все модели реализованы и зарегистрированы
- [ ] Единый API (fit/predict/save/load)
- [ ] Feature importances для всех
- [ ] Дефолтные конфиги для всех моделей
- [ ] GPU support где возможно
- [ ] Тесты для каждой модели

## Промпты

### Промпт 1: Tree-based модели
```
Реализуй в src/modeling/models/tree_based/:
- lightgbm_model.py
- xgboost_model.py
- catboost_model.py
- random_forest_model.py
- extra_trees_model.py

Каждая модель:
- Наследует BaseModel
- Регистрируется в ModelRegistry
- Поддерживает classification и regression
- Early stopping на validation
- Feature importances
- GPU support (где доступно)
- Дефолтные hyperparams

Создай configs/models/ для каждой модели.
```

### Промпт 2: Linear и Tabular NN
```
Реализуй:
1. src/modeling/models/linear/ - LogisticRegression, ElasticNet обёртки
2. src/modeling/models/neural/tabular/ - TabNet, FT-Transformer, NODE

Tabular NN требуют:
- PyTorch implementation
- Training loop с callbacks
- Batch processing
- GPU support
- Early stopping

Используй готовые библиотеки где возможно (pytorch-tabnet, etc).
```

## Важные замечания

- **LightGBM** - основная модель, приоритет
- **GPU**: LightGBM и XGBoost поддерживают GPU
- **Determinism**: Фиксация seeds для reproducibility
- **Categorical features**: CatBoost нативно поддерживает

## Следующий этап
[Этап 08: Нейросетевые модели](Этап_08_Нейросетевые_модели.md)
