# Этап 02: Обработка и валидация данных

## Цель этапа
Реализовать систему фильтрации, клининга и валидации данных для обеспечения качества входных данных перед обучением моделей.

## Зависимости
- Этап 00 (базовая инфраструктура)
- Этап 01 (модуль данных)

## Описание задач

### 1. Структура модуля

```
src/data/
├── filters/                      # Фильтры данных
│   ├── __init__.py
│   ├── base.py                   # Базовый класс Filter
│   ├── anomaly.py                # Фильтр аномалий
│   ├── liquidity.py              # Фильтр по ликвидности
│   ├── events.py                 # Фильтр событий
│   ├── outliers.py               # Фильтр выбросов
│   └── composite.py              # Композиция фильтров
├── cleaners/                     # Клининг данных
│   ├── __init__.py
│   ├── missing_data.py           # Обработка пропусков
│   ├── duplicates.py             # Дедупликация
│   ├── corrections.py            # Исправление данных
│   └── normalization.py          # Нормализация
└── quality/                      # Оценка качества
    ├── __init__.py
    ├── metrics.py                # Метрики качества
    ├── reports.py                # Отчёты
    └── comparator.py             # Сравнение датасетов
```

### 2. Базовый класс Filter

**src/data/filters/base.py:**

```python
class DataFilter(ABC):
    """Базовый класс для фильтров данных"""

    def __init__(self, config: dict):
        self.config = config
        self.stats = FilterStatistics()

    @abstractmethod
    def filter(self, data: pd.DataFrame) -> pd.DataFrame:
        """Применить фильтр к данным"""
        pass

    @abstractmethod
    def get_filter_mask(self, data: pd.DataFrame) -> pd.Series:
        """Получить маску фильтрации (True = оставить)"""
        pass

    def log_statistics(self) -> None:
        """Логировать статистику фильтрации"""
        pass
```

**FilterStatistics:**
- rows_before: int
- rows_after: int
- rows_filtered: int
- filter_percentage: float
- reasons: dict[str, int]  # причина -> количество

### 3. Фильтры аномалий (anomaly.py)

**PriceAnomalyFilter:**
Детектирует и фильтрует аномальные скачки цен.

Методы детекции:
- **Z-score**: выбросы по стандартным отклонениям returns
- **IQR**: межквартильный размах
- **EWMA**: отклонение от exponential moving average
- **Isolation Forest**: ML-based детекция

Параметры:
- method: 'zscore' | 'iqr' | 'ewma' | 'isolation_forest'
- threshold: float (для zscore - количество std, для iqr - коэффициент)
- window: int (для rolling stats)
- action: 'remove' | 'replace' | 'mark'  # что делать с аномалиями

**VolumeAnomalyFilter:**
Детектирует аномальные объёмы торгов.

- Нулевые объёмы
- Отрицательные объёмы
- Внезапные всплески (> N*mean)
- Аномально низкие объёмы

**SpreadAnomalyFilter:**
Фильтрует бары с подозрительным spread (high-low).

- Нулевой spread (все OHLC равны)
- Слишком большой spread (> N% от цены)
- Negative spread (некорректные данные)

### 4. Фильтр ликвидности (liquidity.py)

**LiquidityFilter:**
Удаляет периоды с низкой ликвидностью.

Критерии:
- Минимальный объём за период
- Минимальное количество сделок (если есть данные)
- Минимальный turnover
- Spread < max_spread_pct

Особенности:
- Настраиваемые пороги для разных тикеров
- Учёт времени суток (торговые сессии)
- Учёт дней недели

**TradingHoursFilter:**
Фильтрует данные вне торговых часов.

- Учёт расписания биржи
- Обработка праздников
- Pre-market / After-hours опционально

### 5. Фильтр событий (events.py)

**EventFilter:**
Удаляет или маркирует периоды вокруг значимых событий.

События:
- Сплиты акций
- Дивиденды
- Корпоративные действия
- Новости (если есть данные)
- Экстремальные рыночные события

Параметры:
- event_window: timedelta (окно вокруг события)
- action: 'remove' | 'mark'
- events_file: путь к файлу с событиями

**RolloverFilter:**
Обработка роллов фьючерсов (если применимо).

### 6. Фильтр выбросов (outliers.py)

**StatisticalOutlierFilter:**
Статистическая фильтрация выбросов.

Методы:
- MAD (Median Absolute Deviation)
- Tukey's fences (Q1 - 1.5*IQR, Q3 + 1.5*IQR)
- Grubbs test
- Chauvenet's criterion

**RollingOutlierFilter:**
Rolling window детекция выбросов.

- Адаптивные пороги
- Учёт волатильности
- Минимальное количество наблюдений в окне

### 7. Композиция фильтров (composite.py)

**FilterPipeline:**
Последовательное применение нескольких фильтров.

```python
pipeline = FilterPipeline([
    PriceAnomalyFilter(threshold=3.0),
    VolumeAnomalyFilter(min_volume=1000),
    LiquidityFilter(min_volume=5000),
    StatisticalOutlierFilter(method='mad')
])

filtered_data = pipeline.apply(data)
statistics = pipeline.get_statistics()
```

Возможности:
- Последовательное применение
- Логирование каждого этапа
- Сбор совокупной статистики
- Визуализация результатов

**ConditionalFilter:**
Применение фильтра по условию.

```python
# Фильтровать только в торговые часы
filter = ConditionalFilter(
    condition=lambda df: df['hour'].between(10, 18),
    filter_obj=LiquidityFilter(...)
)
```

### 8. Обработка пропусков (missing_data.py)

**MissingDataHandler:**
Стратегии обработки missing data.

Методы:
- **drop**: удалить строки с NaN
- **forward_fill**: заполнить предыдущим значением
- **backward_fill**: заполнить следующим значением
- **interpolate**: линейная/полиномиальная интерполяция
- **mean/median**: заполнить средним/медианой

Особенности для OHLCV:
- forward_fill для цен (предположение: цена не меняется)
- zero для объёма (если нет торгов)
- Не заполнять слишком большие пропуски

**GapDetector:**
Детектирует пропуски в таймсерии.

- Ожидаемая частота vs фактическая
- Классификация пропусков (малые/большие)
- Анализ паттернов пропусков

### 9. Дедупликация (duplicates.py)

**DuplicateHandler:**
Обработка дубликатов timestamp.

Стратегии разрешения:
- **first**: оставить первый
- **last**: оставить последний
- **mean**: усреднить
- **validate**: raise error если есть дубликаты

Проверки:
- Точные дубликаты (все поля одинаковы)
- Частичные дубликаты (timestamp одинаковый, но данные разные)

### 10. Исправление данных (corrections.py)

**DataCorrector:**
Автоматическое исправление распространённых ошибок.

Исправления:
- Инвертированные OHLC (high < low)
- Отрицательные цены
- Отрицательные объёмы
- Десятичные сдвиги (100x, 1000x ошибки)

**PriceConsistencyChecker:**
Проверка консистентности цен между барами.

- Максимальное изменение цены между барами
- Проверка gap-ов
- Сравнение с другими источниками (если доступно)

### 11. Метрики качества (quality/metrics.py)

**DataQualityMetrics:**
Вычисление метрик качества данных.

Метрики:
- **Completeness**: % присутствующих данных
- **Validity**: % валидных записей
- **Accuracy**: отклонение от эталона (если есть)
- **Consistency**: консистентность OHLC relationships
- **Timeliness**: актуальность данных
- **Uniqueness**: отсутствие дубликатов

**QualityScore:**
Агрегированная оценка качества (0-100).

Веса компонентов настраиваются.

### 12. Отчёты о качестве (quality/reports.py)

**QualityReport:**
Детальный отчёт о качестве данных.

Содержимое:
- Основные статистики (min, max, mean, std)
- Результаты валидации
- Детектированные проблемы
- Применённые фильтры и их эффект
- Визуализации (графики цен, объёмов, распределений)
- Рекомендации по улучшению

Форматы:
- HTML (интерактивный)
- PDF
- JSON (для автоматической обработки)

**ComparisonReport:**
Сравнение качества нескольких датасетов.

### 13. Интеграция в пайплайн

**Обновить DataPreparationPipeline:**

Добавить этапы:
1. Базовая валидация (Этап 01)
2. **Фильтрация аномалий**
3. **Обработка пропусков**
4. **Дедупликация**
5. **Исправление ошибок**
6. **Оценка качества**
7. Сохранение

Каждый этап:
- Логируется
- Можно включить/выключить через конфиг
- Сохраняет статистику
- Может быть skipped если данные уже чистые

### 14. Конфигурация фильтров

**configs/data/filters.yaml:**

```yaml
filters:
  price_anomaly:
    enabled: true
    method: zscore
    threshold: 3.0
    window: 100
    action: remove

  volume_anomaly:
    enabled: true
    min_volume: 1000
    max_volume_spike: 10.0  # 10x mean
    action: mark

  liquidity:
    enabled: true
    min_volume: 5000
    min_turnover: 10000
    max_spread_pct: 5.0

  outliers:
    enabled: true
    method: mad
    threshold: 3.5

  missing_data:
    method: forward_fill
    max_gap: 5  # bars

  duplicates:
    strategy: last
```

### 15. CLI команды

**Расширить data_commands.py:**

```bash
# Фильтрация датасета
python -m src.interfaces.cli data filter-dataset \
  --input SBER/1m \
  --output SBER/1m_filtered \
  --config configs/data/filters.yaml

# Проверка качества
python -m src.interfaces.cli data quality-report \
  --dataset SBER/1m \
  --output artifacts/reports/sber_quality.html

# Сравнение датасетов
python -m src.interfaces.cli data compare-datasets \
  --datasets SBER/1m SBER/5m \
  --output artifacts/reports/comparison.html
```

## Критерии проверки готовности

### Обязательные:
- [ ] Все типы фильтров реализованы
- [ ] FilterPipeline работает
- [ ] Обработка пропусков корректна
- [ ] Дедупликация работает для всех стратегий
- [ ] DataCorrector исправляет распространённые ошибки
- [ ] QualityMetrics вычисляются
- [ ] QualityReport генерируется (HTML, JSON)
- [ ] Интеграция в DataPreparationPipeline
- [ ] CLI команды работают
- [ ] Конфигурация через YAML

### Тесты (tests/unit/data/):
- [ ] test_filters.py - все фильтры с edge cases
- [ ] test_missing_data.py - обработка пропусков
- [ ] test_duplicates.py - дедупликация
- [ ] test_corrections.py - исправление ошибок
- [ ] test_quality_metrics.py - метрики
- [ ] test_filter_pipeline.py - композиция фильтров

### Интеграционные тесты:
- [ ] test_full_cleaning_pipeline.py - полный цикл клининга

### Производительность:
- [ ] Фильтрация 1M баров < 5 секунд
- [ ] Генерация quality report < 10 секунд

## Промпты для реализации

### Промпт 1: Базовые классы и фильтры аномалий
```
Реализуй базовый класс для фильтров и фильтры аномалий:

1. src/data/filters/base.py:
   - Абстрактный DataFilter класс
   - FilterStatistics dataclass
   - Методы: filter(), get_filter_mask(), log_statistics()

2. src/data/filters/anomaly.py:
   - PriceAnomalyFilter (zscore, iqr, ewma методы)
   - VolumeAnomalyFilter
   - SpreadAnomalyFilter

Каждый фильтр должен:
- Наследоваться от DataFilter
- Быть конфигурируемым через dict
- Логировать статистику фильтрации
- Поддерживать разные действия (remove, replace, mark)

Используй numpy/pandas vectorized операции для производительности.
Добавь подробные docstrings.
```

### Промпт 2: Фильтры ликвидности и событий
```
Реализуй фильтры в src/data/filters/:

1. liquidity.py:
   - LiquidityFilter: фильтр по объёму, turnover, spread
   - TradingHoursFilter: учёт торговых часов биржи
   - Поддержка разных тикеров с разными порогами

2. events.py:
   - EventFilter: фильтрация вокруг событий
   - Загрузка событий из файла (CSV, JSON)
   - Окно фильтрации настраивается
   - RolloverFilter для фьючерсов (заглушка пока)

3. outliers.py:
   - StatisticalOutlierFilter (MAD, Tukey, Grubbs)
   - RollingOutlierFilter с адаптивными порогами

Все фильтры должны собирать детальную статистику.
```

### Промпт 3: Композиция фильтров
```
Реализуй в src/data/filters/composite.py:

1. FilterPipeline:
   - Список фильтров для последовательного применения
   - apply(data) метод
   - get_statistics() - совокупная статистика
   - visualize_effects() - визуализация эффекта каждого фильтра

2. ConditionalFilter:
   - Применение фильтра по условию
   - condition: Callable[[DataFrame], Series[bool]]
   - Логирование когда условие выполняется

3. ParallelFilterGroup:
   - Применение нескольких фильтров параллельно
   - Объединение результатов (AND/OR логика)

Добавь примеры использования в docstrings.
```

### Промпт 4: Обработка пропусков и дубликатов
```
Реализуй в src/data/cleaners/:

1. missing_data.py:
   - MissingDataHandler с методами: drop, forward_fill, backward_fill, interpolate
   - GapDetector: детекция и анализ пропусков
   - Специальная логика для OHLCV (forward_fill для цен, zero для объёма)
   - Ограничение на max_gap

2. duplicates.py:
   - DuplicateHandler с стратегиями: first, last, mean, validate
   - Детекция точных и частичных дубликатов
   - Логирование всех найденных дубликатов

Обе функции должны быть idempotent.
Добавь валидацию результата после обработки.
```

### Промпт 5: Исправление данных
```
Реализуй в src/data/cleaners/corrections.py:

1. DataCorrector:
   - fix_inverted_ohlc(): исправить high < low
   - fix_negative_values(): заменить отрицательные на NaN
   - fix_decimal_errors(): детектировать 10x, 100x, 1000x ошибки
   - apply_all_corrections(): применить все исправления

2. PriceConsistencyChecker:
   - check_max_change(max_pct): проверить максимальное изменение между барами
   - check_gaps(): анализ гэпов
   - validate_consistency(): общая проверка

Логировать все исправления с деталями.
Создать отчёт о всех исправленных проблемах.
```

### Промпт 6: Метрики и отчёты качества
```
Реализуй в src/data/quality/:

1. metrics.py - DataQualityMetrics:
   - calculate_completeness(data)
   - calculate_validity(data)
   - calculate_consistency(data)
   - calculate_uniqueness(data)
   - calculate_quality_score(data) - агрегированный score

2. reports.py:
   - QualityReport класс:
     * generate_html_report(data, output_path)
     * generate_json_report(data, output_path)
     * Включить: stats, charts (matplotlib/plotly), issues, recommendations

   - ComparisonReport:
     * compare_datasets(datasets: list[DataFrame])
     * Визуальное сравнение метрик

Используй jinja2 для HTML templates.
Добавь интерактивные графики (plotly).
```

### Промпт 7: Интеграция и CLI
```
1. Обнови src/pipelines/data_preparation.py:
   - Добавь этап фильтрации после базовой валидации
   - Добавь этап клининга (пропуски, дубликаты, исправления)
   - Добавь генерацию quality report
   - Каждый этап настраивается через config
   - Сохрани все статистики в metadata

2. Расширь src/interfaces/cli/data_commands.py:
   - filter_dataset() - применить фильтры
   - quality_report() - сгенерировать отчёт
   - compare_datasets() - сравнить датасеты
   - clean_dataset() - full cleaning pipeline

Используй click для CLI.
Добавь progress bars и красивый вывод.
```

### Промпт 8: Тесты
```
Создай тесты:

tests/unit/data/filters/:
- test_anomaly_filters.py - все методы детекции
- test_liquidity_filter.py
- test_outlier_filters.py
- test_filter_pipeline.py

tests/unit/data/cleaners/:
- test_missing_data.py - все стратегии
- test_duplicates.py
- test_corrections.py

tests/unit/data/quality/:
- test_metrics.py
- test_reports.py - генерация отчётов

tests/integration/data/:
- test_cleaning_pipeline.py - полный цикл
- test_filter_effects.py - эффект фильтров на downstream tasks

Используй pytest, fixtures, parametrize.
Проверь edge cases (пустые данные, все NaN, все дубликаты).
```

## Важные замечания

### Статистика фильтрации:
Всегда логировать:
- Сколько строк отфильтровано
- Процент от исходных данных
- Причины фильтрации
- Влияние на downstream метрики

### Идемпотентность:
Повторное применение фильтров не должно изменять уже отфильтрованные данные.

### Конфигурируемость:
Все пороги и параметры через конфиг, не хардкодить.

### Визуализация:
Важно визуализировать эффект фильтров:
- До/после графики
- Распределения
- Детектированные аномалии

### Производительность:
- Векторизация операций
- Избегать циклов по строкам
- Для больших данных - chunked processing

### Look-ahead bias:
Фильтры не должны использовать будущую информацию.
Rolling stats должны быть causal.

## Следующий этап
После завершения переходите к [Этапу 03: Библиотека индикаторов](Этап_03_Библиотека_индикаторов.md)
