# Этап 01: Модуль данных (Data Ingestion)

## Цель этапа
Реализовать модуль для загрузки, хранения и каталогизации исторических данных из локальных файлов и подготовить основу для интеграции с Tinkoff API.

## Зависимости
- Этап 00 (базовая инфраструктура должна быть готова)

## Описание задач

### 1. Структура модуля данных

```
src/data/
├── __init__.py
├── loaders/                      # Загрузчики данных
│   ├── __init__.py
│   ├── base.py                   # Базовый класс
│   ├── local_file.py             # Из локальных файлов
│   ├── tinkoff_stub.py           # Заглушка для Tinkoff API
│   └── cached.py                 # С кэшированием
├── storage/                      # Хранение данных
│   ├── __init__.py
│   ├── parquet_storage.py        # Parquet хранилище
│   ├── catalog.py                # Каталог датасетов
│   └── versioning.py             # Версионирование
├── preprocessors/                # Предобработка
│   ├── __init__.py
│   ├── timezone.py               # Работа с таймзонами
│   ├── resampler.py              # Ресэмплинг таймфреймов
│   └── merger.py                 # Объединение источников
├── validators/                   # Валидация
│   ├── __init__.py
│   ├── integrity.py              # Проверка целостности
│   ├── schema.py                 # Проверка схемы
│   └── quality.py                # Проверка качества
└── schemas.py                    # Pydantic схемы
```

### 2. Схемы данных (schemas.py)

**Основные модели:**

1. **OHLCVBar** - единичный бар:
   - timestamp (datetime with timezone)
   - ticker (str)
   - open, high, low, close (float)
   - volume (int)
   - Валидация: high >= max(open, close), low <= min(open, close), volume >= 0

2. **DatasetMetadata** - метаданные датасета:
   - dataset_id (UUID)
   - ticker (str)
   - timeframe (str: '1m', '5m', '15m', '1h', '4h', '1d')
   - start_date, end_date (datetime)
   - source (str: 'local', 'tinkoff', 'manual')
   - timezone (str)
   - total_bars (int)
   - missing_bars (int)
   - hash (str: SHA256 хэш данных)
   - created_at (datetime)
   - schema_version (str)

3. **DatasetConfig** - конфигурация загрузки:
   - ticker (str или list[str])
   - timeframe (str)
   - from_date, to_date (date)
   - source_type ('local' | 'api')
   - file_path (optional)
   - resample_from (optional: загрузить 1m и ресэмплировать)
   - timezone (default 'UTC')
   - validate_data (bool, default True)

### 3. Загрузчики данных (loaders/)

**base.py - базовый интерфейс:**
```python
from typing import Protocol
import pandas as pd

class DataLoader(Protocol):
    """Интерфейс для загрузчиков данных"""

    def load(
        self,
        ticker: str,
        from_date: datetime,
        to_date: datetime,
        timeframe: str = '1m',
        **kwargs
    ) -> pd.DataFrame:
        """
        Загрузить OHLCV данные

        Returns:
            DataFrame с колонками: timestamp, ticker, open, high, low, close, volume
        """
        ...

    def get_available_tickers(self) -> list[str]:
        """Получить список доступных тикеров"""
        ...
```

**local_file.py - загрузка из локальных файлов:**
- Поддержка форматов: Parquet, CSV
- Автоматическое определение формата
- Валидация схемы при загрузке
- Обработка разных форматов дат
- Автоматическая конвертация таймзон

**cached.py - обёртка с кэшированием:**
- Кэш на диске (Parquet)
- LRU cache в памяти для часто используемых данных
- Инвалидация кэша по времени или по изменению источника
- Статистика использования кэша

**tinkoff_stub.py - заглушка для Tinkoff API:**
- Реализует интерфейс DataLoader
- На данном этапе возвращает пустые данные или raise NotImplementedError
- Будет реализовано в Этапе 16

### 4. Хранилище (storage/)

**parquet_storage.py - работа с Parquet файлами:**

Функции:
- `save_dataset()` - сохранить датасет с метаданными
- `load_dataset()` - загрузить датасет
- `append_data()` - добавить новые данные к существующему датасету
- `list_datasets()` - список всех датасетов
- `get_metadata()` - получить метаданные датасета
- `delete_dataset()` - удалить датасет

Структура хранения:
```
artifacts/data/
├── SBER/
│   ├── 1m/
│   │   ├── 2020.parquet
│   │   ├── 2021.parquet
│   │   └── metadata.json
│   ├── 5m/
│   │   └── ...
│   └── 1d/
│       └── ...
└── GAZP/
    └── ...
```

**catalog.py - каталог датасетов:**
- Централизованный реестр всех датасетов
- SQLite база данных для быстрого поиска
- Индексы по ticker, timeframe, date range
- API для поиска и фильтрации датасетов
- Автоматическое обновление при добавлении новых данных

**versioning.py - версионирование данных:**
- Вычисление хэша датасета (SHA256)
- Сравнение версий (detect изменений)
- История изменений датасета
- Возможность откатиться к предыдущей версии

### 5. Предобработка (preprocessors/)

**timezone.py - работа с таймзонами:**
- Конвертация в UTC
- Валидация что все timestamps имеют timezone info
- Обработка перехода на летнее/зимнее время
- Функции для конвертации между таймзонами

**resampler.py - ресэмплинг таймфреймов:**
- Из 1m → 5m, 15m, 1h, 4h, 1d
- Корректная агрегация OHLCV:
  - open: первое значение
  - high: максимум
  - low: минимум
  - close: последнее значение
  - volume: сумма
- Обработка неполных баров (например, последний бар дня)
- Выравнивание по началу периода

**merger.py - объединение данных из разных источников:**
- Merge данных по timestamp
- Разрешение конфликтов (какой источник приоритетнее)
- Заполнение пропусков
- Валидация консистентности после merge

### 6. Валидация (validators/)

**integrity.py - проверка целостности:**
- Проверка на дубликаты timestamp
- Проверка на пропуски (missing bars)
- Проверка монотонности timestamp
- Проверка полноты данных (все колонки присутствуют)

**schema.py - проверка схемы:**
- Проверка типов данных колонок
- Проверка обязательных колонок
- Валидация диапазонов значений (OHLC relationships)
- Проверка формата timestamp

**quality.py - проверка качества:**
- Детекция аномалий в ценах (внезапные скачки)
- Детекция нулевых/отрицательных объёмов
- Проверка spread (high-low)
- Статистический анализ качества данных
- Генерация отчёта о качестве

### 7. Пайплайн загрузки данных

**Создать src/pipelines/data_preparation.py:**

Класс `DataPreparationPipeline`:
```python
class DataPreparationPipeline:
    """
    Пайплайн подготовки данных:
    1. Загрузка из источника
    2. Валидация
    3. Предобработка (timezone, resample)
    4. Сохранение в storage
    5. Обновление каталога
    """

    def __init__(self, config: DatasetConfig):
        self.config = config
        self.loader = self._create_loader()
        self.storage = ParquetStorage()
        self.catalog = DatasetCatalog()

    def run(self) -> DatasetMetadata:
        """Выполнить пайплайн"""
        # 1. Load
        data = self.loader.load(...)

        # 2. Validate
        self.validate(data)

        # 3. Preprocess
        data = self.preprocess(data)

        # 4. Save
        metadata = self.storage.save_dataset(data, ...)

        # 5. Update catalog
        self.catalog.add_dataset(metadata)

        return metadata
```

### 8. Команды CLI

**src/interfaces/cli/data_commands.py:**

Команды:
- `load-data` - загрузить данные (из файла или API)
- `list-datasets` - список всех датасетов
- `validate-dataset` - проверить датасет
- `resample-dataset` - ресэмплировать датасет
- `dataset-info` - информация о датасете (метаданные, статистика)
- `export-dataset` - экспортировать в другой формат

Пример:
```bash
python -m src.interfaces.cli data load-data \
  --ticker SBER \
  --from-date 2020-01-01 \
  --to-date 2023-12-31 \
  --timeframe 1m \
  --source local \
  --file data/sber_1m.parquet

python -m src.interfaces.cli data list-datasets

python -m src.interfaces.cli data resample-dataset \
  --input SBER/1m \
  --output SBER/5m \
  --target-timeframe 5m
```

## Критерии проверки готовности

### Обязательные:
- [ ] Все классы и модули реализованы
- [ ] Pydantic схемы для всех основных сущностей
- [ ] DataLoader интерфейс реализован для локальных файлов
- [ ] Можно загрузить данные из CSV/Parquet файла
- [ ] Ресэмплинг работает корректно (1m → 5m, 1h, 1d)
- [ ] Валидация данных работает и детектит проблемы
- [ ] Хранилище сохраняет данные с правильной структурой
- [ ] Каталог датасетов работает (add, search, list)
- [ ] Версионирование вычисляет корректные хэши
- [ ] CLI команды работают
- [ ] Логирование всех операций

### Тесты (tests/unit/data/):
- [ ] test_schemas.py - валидация Pydantic моделей
- [ ] test_local_loader.py - загрузка из файлов
- [ ] test_resampler.py - корректность ресэмплинга
- [ ] test_validators.py - все виды валидации
- [ ] test_storage.py - сохранение/загрузка
- [ ] test_catalog.py - работа каталога
- [ ] test_versioning.py - хэширование и версионирование

### Интеграционные тесты (tests/integration/):
- [ ] test_data_pipeline.py - полный цикл загрузки и сохранения

### Производительность:
- [ ] Загрузка 1M баров < 5 секунд
- [ ] Ресэмплинг 1M баров < 10 секунд
- [ ] Валидация 1M баров < 3 секунды

## Промпты для реализации

### Промпт 1: Схемы данных
```
Реализуй Pydantic схемы для модуля данных в src/data/schemas.py:

1. OHLCVBar - схема для одного OHLCV бара:
   - timestamp: datetime с timezone
   - ticker: str
   - open, high, low, close: float (с валидацией соотношений)
   - volume: int (неотрицательный)
   - Кастомные валидаторы:
     * high >= max(open, close)
     * low <= min(open, close)
     * volume >= 0

2. DatasetMetadata - метаданные датасета:
   - dataset_id: UUID
   - ticker, timeframe: str
   - start_date, end_date: datetime
   - source: Literal['local', 'tinkoff', 'manual']
   - timezone: str
   - total_bars, missing_bars: int
   - hash: str (SHA256)
   - created_at: datetime
   - schema_version: str

3. DatasetConfig - конфигурация загрузки данных:
   - ticker: str | list[str]
   - timeframe: Literal['1m', '5m', '15m', '1h', '4h', '1d']
   - from_date, to_date: date
   - source_type: Literal['local', 'api']
   - file_path: Optional[Path]
   - resample_from: Optional[str]
   - timezone: str = 'UTC'
   - validate_data: bool = True

Используй Field для документации, валидаторы где нужно.
Следуй стандартам из docs/system/Development_Standards.md.
```

### Промпт 2: Загрузчики данных
```
Реализуй загрузчики данных в src/data/loaders/:

1. base.py - DataLoader Protocol:
   - load() метод с сигнатурой
   - get_available_tickers()
   - Docstrings с описанием контракта

2. local_file.py - LocalFileLoader:
   - Загрузка из Parquet и CSV
   - Автоопределение формата файла
   - Парсинг разных форматов дат
   - Валидация схемы
   - Конвертация timezone в UTC
   - Обработка ошибок (файл не найден, неверный формат)

3. cached.py - CachedDataLoader (decorator):
   - Обёртка над любым DataLoader
   - Кэш на диске (Parquet в artifacts/cache/)
   - LRU cache в памяти (lru_cache)
   - TTL для инвалидации
   - Логирование cache hits/misses

4. tinkoff_stub.py - TinkoffDataLoader:
   - Заглушка, raise NotImplementedError
   - Оставь TODO комментарии для будущей реализации

Используй type hints, логирование, обработку ошибок.
Протестируй на примерах CSV и Parquet файлов.
```

### Промпт 3: Хранилище и каталог
```
Реализуй систему хранения в src/data/storage/:

1. parquet_storage.py - ParquetStorage:
   - save_dataset(data, metadata) → DatasetMetadata
   - load_dataset(ticker, timeframe, from_date, to_date) → DataFrame
   - append_data(dataset_id, new_data)
   - list_datasets(ticker=None, timeframe=None) → list[DatasetMetadata]
   - delete_dataset(dataset_id)

   Структура на диске:
   artifacts/data/{ticker}/{timeframe}/{year}.parquet
   artifacts/data/{ticker}/{timeframe}/metadata.json

2. catalog.py - DatasetCatalog:
   - SQLite база в artifacts/db/catalog.db
   - Таблица datasets с полями из DatasetMetadata
   - Методы: add, search, filter, get_by_id
   - Индексы для быстрого поиска

3. versioning.py - DataVersioning:
   - compute_hash(dataframe) → str (SHA256)
   - compare_versions(hash1, hash2) → dict
   - get_history(dataset_id) → list[Version]
   - save_version_snapshot(dataset_id, data)

Обеспечь thread-safety для каталога.
Используй логирование всех операций.
```

### Промпт 4: Предобработка данных
```
Реализуй препроцессоры в src/data/preprocessors/:

1. timezone.py:
   - convert_to_utc(data: DataFrame) → DataFrame
   - validate_timezone_aware(data) → bool
   - localize_timestamp(ts, tz) → datetime
   - handle_dst_transition(data) - обработка перехода на летнее время

2. resampler.py - TimeframeResampler:
   - resample(data, from_tf, to_tf) → DataFrame
   - Агрегация OHLCV: open=first, high=max, low=min, close=last, volume=sum
   - Выравнивание по началу периода
   - Обработка неполных баров
   - Поддержка: 1m→5m, 1m→15m, 1m→1h, 1m→4h, 1m→1d

3. merger.py - DataMerger:
   - merge_sources(sources: list[DataFrame], strategy) → DataFrame
   - resolve_conflicts(data, priority_order)
   - fill_gaps(data, method='forward_fill' | 'interpolate')
   - validate_after_merge(data)

Все функции должны быть idempotent.
Добавь проверки корректности агрегации.
```

### Промпт 5: Валидация данных
```
Реализуй валидаторы в src/data/validators/:

1. integrity.py - IntegrityValidator:
   - check_duplicates(data) → ValidationResult
   - check_missing_bars(data, timeframe) → ValidationResult
   - check_monotonic_timestamps(data) → ValidationResult
   - check_completeness(data) → ValidationResult

2. schema.py - SchemaValidator:
   - validate_columns(data, required_columns) → ValidationResult
   - validate_dtypes(data) → ValidationResult
   - validate_ohlc_relationships(data) → ValidationResult
   - validate_timestamp_format(data) → ValidationResult

3. quality.py - QualityValidator:
   - detect_price_anomalies(data, threshold=3.0) → ValidationResult
   - check_volume_sanity(data) → ValidationResult
   - check_spread(data, max_spread_pct=10.0) → ValidationResult
   - generate_quality_report(data) → dict

ValidationResult должен содержать:
- is_valid: bool
- errors: list[str]
- warnings: list[str]
- statistics: dict

Используй статистические методы для детекции аномалий.
```

### Промпт 6: Пайплайн и CLI
```
Реализуй:

1. src/pipelines/data_preparation.py - DataPreparationPipeline:
   - __init__(config: DatasetConfig)
   - run() → DatasetMetadata
   - Этапы:
     a. Загрузка через loader
     b. Валидация всеми валидаторами
     c. Предобработка (timezone, resample если нужно)
     d. Сохранение в storage
     e. Регистрация в catalog
   - Логирование каждого этапа
   - Обработка ошибок с rollback
   - Возможность перезапуска с чекпоинтами

2. src/interfaces/cli/data_commands.py - CLI команды:
   - load_data() - загрузить данные
   - list_datasets() - показать список
   - validate_dataset() - проверить
   - resample_dataset() - ресэмплировать
   - dataset_info() - информация
   - export_dataset() - экспорт

Используй click для CLI.
Добавь progress bars (tqdm).
Красивый вывод в терминал (rich или colorama).
```

### Промпт 7: Тесты
```
Создай тесты для модуля данных:

tests/unit/data/:
- test_schemas.py - валидация Pydantic моделей, edge cases
- test_local_loader.py - загрузка разных форматов
- test_resampler.py - корректность ресэмплинга с известными данными
- test_validators.py - все валидаторы
- test_storage.py - save/load/append
- test_catalog.py - CRUD операции
- test_versioning.py - хэши и версии

tests/integration/data/:
- test_data_pipeline.py - полный цикл от файла до storage

Используй:
- pytest fixtures для тестовых данных
- parametrize для разных сценариев
- tmp_path для временных файлов
- mock для external dependencies

Покрытие должно быть > 80%.
```

## Важные замечания

### Форматы данных:
- **Основной формат**: Parquet (быстрый, эффективный)
- **Поддержка CSV**: Для совместимости
- **Encoding**: UTF-8
- **Timezone**: Всегда UTC в хранилище, конвертация при загрузке/выгрузке

### Структура DataFrame:
Стандартные колонки (порядок важен):
```python
['timestamp', 'ticker', 'open', 'high', 'low', 'close', 'volume']
```
- timestamp: pd.DatetimeIndex с timezone
- ticker: str
- OHLC: float64
- volume: int64

### Оптимизация производительности:
- Используйте pandas vectorized операции
- Избегайте iterrows(), используйте apply() или numba
- Для больших файлов - chunked чтение
- Parquet с compression='snappy'

### Обработка ошибок:
- Валидационные ошибки → ValidationError с подробным описанием
- IO ошибки → DataLoadError
- Все ошибки логировать с контекстом
- Graceful degradation где возможно

### Идемпотентность:
- Повторный запуск загрузки должен детектить существующие данные
- Использовать хэши для определения изменений
- Append только новых данных, не дублировать

### Версионирование:
- Schema version для обратной совместимости
- При изменении схемы - миграции

### Логирование:
```python
logger.info("Loading data", extra={
    'ticker': ticker,
    'from_date': from_date,
    'to_date': to_date,
    'timeframe': timeframe,
    'source': source_type
})
```

### Примеры использования:
```python
# Загрузка из файла
config = DatasetConfig(
    ticker='SBER',
    timeframe='1m',
    from_date=date(2020, 1, 1),
    to_date=date(2023, 12, 31),
    source_type='local',
    file_path=Path('data/sber.parquet')
)

pipeline = DataPreparationPipeline(config)
metadata = pipeline.run()

# Ресэмплинг
resampler = TimeframeResampler()
data_5m = resampler.resample(data_1m, '1m', '5m')

# Поиск в каталоге
catalog = DatasetCatalog()
datasets = catalog.search(ticker='SBER', timeframe='5m')
```

## Следующий этап
После завершения переходите к [Этапу 02: Обработка и валидация данных](Этап_02_Обработка_и_валидация_данных.md)
