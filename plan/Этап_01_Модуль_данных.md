# Этап 01: Модуль данных (Data Ingestion)

## Цель этапа
Реализовать модуль для загрузки, хранения и каталогизации исторических данных из локальных файлов и подготовить основу для интеграции с Tinkoff API.

## Зависимости
- Этап 00 (базовая инфраструктура должна быть готова)

## Описание задач

### 1. Структура модуля данных

```

Ключевые моменты реализации:
- Используем `asyncio` + `aiohttp`/`httpx` или аналог для параллельной загрузки архивов, а также многопоточную/асинхронную обработку следующих шагов (распаковка, конвертация, ресэмплинг, валидация) через пулы задач и очереди; соблюдаем лимит 30 unary-запросов в минуту (rate limiter, очереди, ретраи с экспоненциальной паузой).
- Каждый архив содержит ZIP с поддиректориями по дням; распаковку выполняем потоково (не выгружая весь архив в память) в папку `artifacts/raw/extracted/{ticker}/{year}`.
- Сырые CSV приводим к стандартизованной схеме (см. «Форматы данных»), добавляем `ticker`, конвертируем в UTC и считаем контрольные суммы для верификации целостности.
- После конвертации минутных данных автоматически строим таймфреймы 5m/15m/1h/4h/1d, сохраняя результат в структуре `artifacts/data/{ticker}/{timeframe}/{year}.parquet`.
- После каждого шага обновляем прогресс и логируем метрики (количество файлов, скорость скачивания, объём данных).
- Для текущего года выполняем периодическую догрузку новых данных (incremental update), не перезатирая уже обработанные периоды.
- По итогам проверок полноты автоматически планируем докачку пропущенных архивов и повторную обработку только отсутствующих фрагментов.
src/data/
├── __init__.py
├── loaders/                      # Загрузчики данных
│   ├── __init__.py
│   ├── base.py                   # Базовый класс
│   ├── local_file.py             # Из локальных файлов
│   ├── tinkoff_stub.py           # Заглушка для Tinkoff API
│   └── cached.py                 # С кэшированием
├── storage/                      # Хранение данных
│   ├── __init__.py
│   ├── parquet_storage.py        # Parquet хранилище
│   ├── catalog.py                # Каталог датасетов
│   └── versioning.py             # Версионирование
├── preprocessors/                # Предобработка
│   ├── __init__.py
│   ├── timezone.py               # Работа с таймзонами
│   ├── resampler.py              # Ресэмплинг таймфреймов
│   └── merger.py                 # Объединение источников
├── validators/                   # Валидация
│   ├── __init__.py
│   ├── integrity.py              # Проверка целостности
│   ├── schema.py                 # Проверка схемы
│   └── quality.py                # Проверка качества
└── schemas.py                    # Pydantic схемы
```

### 2. Схемы данных (schemas.py)

**Основные модели:**

1. **OHLCVBar** - единичный бар:
   - timestamp (datetime with timezone)
   - ticker (str)
   - open, high, low, close (float)
   - volume (int)
   - Валидация: high >= max(open, close), low <= min(open, close), volume >= 0

2. **DatasetMetadata** - метаданные датасета:
   - dataset_id (UUID)
   - ticker (str)
   - timeframe (str: '1m', '5m', '15m', '1h', '4h', '1d')
   - start_date, end_date (datetime)
   - source (str: 'local', 'tinkoff', 'manual')
   - timezone (str)
   - total_bars (int)
   - missing_bars (int)
   - hash (str: SHA256 хэш данных)
   - created_at (datetime)
   - schema_version (str)

3. **DatasetConfig** - конфигурация загрузки:
   - ticker (str или list[str])
   - tickers_file (Optional[Path]) — путь к файлу со списком тикеров, если загрузка выполняется пакетно
   - timeframe (str)
   - from_date, to_date (date)
   - source_type ('local' | 'api')
   - file_path (optional)
   - resample_from (optional: загрузить 1m и ресэмплировать)
   - timezone (default 'UTC')
   - validate_data (bool, default True)
   - update_latest_year (bool, default True) — периодически обновлять текущий год (неполные данные) без повторного скачивания полных архивов прошлых лет
   - backfill_missing (bool, default True) — автоматически докачивать недостающие архивы по итогам проверки полноты

### 3. Загрузчики данных (loaders/)

**base.py - базовый интерфейс:**
```python
from typing import Protocol
import pandas as pd

class DataLoader(Protocol):
    """Интерфейс для загрузчиков данных"""

    def load(
        self,
        ticker: str,
        from_date: datetime,
        to_date: datetime,
        timeframe: str = '1m',
        **kwargs
    ) -> pd.DataFrame:
        """
        Загрузить OHLCV данные

        Returns:
            DataFrame с колонками: timestamp, ticker, open, high, low, close, volume
        """
        ...

    def get_available_tickers(self) -> list[str]:
        """Получить список доступных тикеров"""
        ...
```

**local_file.py - загрузка из локальных файлов:**
- Поддержка форматов: Parquet, CSV
- Автоматическое определение формата
- Валидация схемы при загрузке
- Обработка разных форматов дат
- Автоматическая конвертация таймзон
- Поддержка парсинга «сырых» CSV из `getHistory`, где столбцы разделены `;` в порядке: `figi;timestamp;open;high;low;close;volume` (+возможный завершающий разделитель)

**cached.py - обёртка с кэшированием:**
- Кэш на диске (Parquet)
- LRU cache в памяти для часто используемых данных
- Инвалидация кэша по времени или по изменению источника
- Статистика использования кэша

**tinkoff_stub.py - заглушка для Tinkoff API:**
- Реализует интерфейс DataLoader
- На данном этапе возвращает пустые данные или raise NotImplementedError
- Будет реализовано в Этапе 16
- Целевая реализация будет использовать REST метод `getHistory` Tinkoff Invest API и обязана учитывать лимит 30 unary-запросов в минуту через встроенный rate limiter, систему повторов и поддержку параллельных/асинхронных загрузок годовых архивов минутных свечей

### 4. Хранилище (storage/)

**parquet_storage.py - работа с Parquet файлами:**

Функции:
- `save_dataset()` - сохранить датасет с метаданными
- `load_dataset()` - загрузить датасет
- `append_data()` - добавить новые данные к существующему датасету
- `list_datasets()` - список всех датасетов
- `get_metadata()` - получить метаданные датасета
- `delete_dataset()` - удалить датасет
- Управление промежуточными артефактами (`downloads`, `extracted`) и регулярная очистка временных файлов

Структура хранения артефактов:
```
artifacts/raw/downloads/
├── SBER/
│   ├── 2020.zip
│   └── 2021.zip
└── GAZP/
    └── ...

artifacts/raw/extracted/
├── SBER/
│   ├── 2020/
│   │   ├── 2020-01-03.csv
│   │   └── ...
│   └── 2021/
│       └── ...
└── GAZP/
    └── ...

artifacts/data/
├── SBER/
│   ├── 1m/
│   │   ├── 2020.parquet
│   │   ├── 2021.parquet
│   │   └── metadata.json
│   ├── 5m/
│   │   ├── 2020.parquet
│   │   └── ...
│   ├── 15m/
│   │   └── ...
│   ├── 1h/
│   │   └── ...
│   ├── 4h/
│   │   └── ...
│   └── 1d/
│       └── ...
└── GAZP/
    └── ...
```

**catalog.py - каталог датасетов:**
- Централизованный реестр всех датасетов
- SQLite база данных для быстрого поиска
- Индексы по ticker, timeframe, date range
- API для поиска и фильтрации датасетов
- Автоматическое обновление при добавлении новых данных

**versioning.py - версионирование данных:**
- Вычисление хэша датасета (SHA256)
- Сравнение версий (detect изменений)
- История изменений датасета
- Возможность откатиться к предыдущей версии

### 5. Предобработка (preprocessors/)

**timezone.py - работа с таймзонами:**
- Конвертация в UTC
- Валидация что все timestamps имеют timezone info
- Обработка перехода на летнее/зимнее время
- Функции для конвертации между таймзонами

**resampler.py - ресэмплинг таймфреймов:**
- Из 1m → 5m, 15m, 1h, 4h, 1d
- Корректная агрегация OHLCV:
  - open: первое значение
  - high: максимум
  - low: минимум
  - close: последнее значение
  - volume: сумма
- Обработка неполных баров (например, последний бар дня)
- Выравнивание по началу периода

**merger.py - объединение данных из разных источников:**
- Merge данных по timestamp
- Разрешение конфликтов (какой источник приоритетнее)
- Заполнение пропусков
- Валидация консистентности после merge

### 6. Валидация (validators/)

**integrity.py - проверка целостности:**
- Проверка на дубликаты timestamp
- Проверка на пропуски (missing bars)
- Проверка монотонности timestamp
- Проверка полноты данных (все колонки присутствуют)
- Проверка покрытия по тикерам/годам (сверка с датой листинга и перечнем обязательных архивов)

**schema.py - проверка схемы:**
- Проверка типов данных колонок
- Проверка обязательных колонок
- Валидация диапазонов значений (OHLC relationships)
- Проверка формата timestamp

**quality.py - проверка качества:**
- Детекция аномалий в ценах (внезапные скачки)
- Детекция нулевых/отрицательных объёмов
- Проверка spread (high-low)
- Статистический анализ качества данных
- Генерация отчёта о качестве

### 7. Пайплайн загрузки данных

**Создать src/pipelines/data_preparation.py:**

Класс `DataPreparationPipeline`:
```python
class DataPreparationPipeline:
    """
    Пайплайн подготовки данных:
    1. Формирование списка тикеров (конфигурация или файл)
    2. Асинхронная параллельная загрузка годовых архивов через getHistory
    3. Параллельная распаковка дневных файлов и нормализация структуры CSV
    4. Асинхронная конвертация в Parquet и построение агрегатов по таймфреймам
    5. Валидация и проверка полноты (тикер/год, структура, качество)
    6. Автоматическая докачка недостающих периодов и обновление текущего года
    7. Сохранение в storage
    8. Обновление каталога
    """

    def __init__(self, config: DatasetConfig):
        self.config = config
        self.loader = self._create_loader()
        self.storage = ParquetStorage()
        self.catalog = DatasetCatalog()

    def run(self) -> DatasetMetadata:
        """Выполнить пайплайн"""
        # 1. Собрать список тикеров и целевых лет
        tickers = self._resolve_tickers()
        listing_map = self._fetch_listing_years(tickers)

        # 2. Асинхронно скачать архивы getHistory и распараллелить распаковку
        raw_paths = await self._download_archives(tickers, listing_map)
        extracted = await self._extract_archives(raw_paths)

        # 3. Асинхронно нормализовать CSV и конвертировать в 1m Parquet
        minute_data = await self._convert_to_parquet(extracted)

        # 4. Выполнить ресэмплинг в старшие таймфреймы параллельными worker'ами
        datasets = await self._resample(minute_data)

        # 5. Проверить полноту по годам/тикерам и инициировать докачку недостающих архивов
        missing = self._detect_missing(listing_map, datasets)
        if missing and self.config.backfill_missing:
            await self._backfill_missing(missing)

        # 6. Обновить данные последнего года (incremental) без повторной обработки прошлых лет
        if self.config.update_latest_year:
            await self._refresh_latest_year(datasets)

        # 7. Провести финальную валидацию структуры и качества
        self.validate(datasets)

        # 8. Сохранить и зарегистрировать датасеты
        metadata = self.storage.save_dataset(datasets, ...)
        self.catalog.add_dataset(metadata)

        return metadata
```

### 8. Команды CLI

**src/interfaces/cli/data_commands.py:**

Команды:
- `load-data` - загрузить данные (из файла или API)
- `list-datasets` - список всех датасетов
- `validate-dataset` - проверить датасет
- `resample-dataset` - ресэмплировать датасет
- `dataset-info` - информация о датасете (метаданные, статистика)
- `export-dataset` - экспортировать в другой формат
- Поддержка опций `--tickers` (список через запятую) и `--tickers-file path/to/file` для массовой выгрузки; автоматическая проверка покрытия по годам после завершения
- Флаги `--update-latest/--no-update-latest` и `--backfill-missing/--no-backfill-missing` управляют догрузкой текущего года и докачкой пропущенных архивов

Пример:
```bash
python -m src.interfaces.cli data load-data \
  --ticker SBER \
  --from-date 2020-01-01 \
  --to-date 2023-12-31 \
  --timeframe 1m \
  --source local \
  --file data/sber_1m.parquet

python -m src.interfaces.cli data load-data \
  --tickers-file config/tickers.txt \
  --source api \
  --rate-limit 30 \
  --output-dir artifacts/data \
  --update-latest \
  --backfill-missing

python -m src.interfaces.cli data list-datasets

python -m src.interfaces.cli data resample-dataset \
  --input SBER/1m \
  --output SBER/5m \
  --target-timeframe 5m
```

## Критерии проверки готовности

### Обязательные:
- [ ] Все классы и модули реализованы
- [ ] Pydantic схемы для всех основных сущностей
- [ ] DataLoader интерфейс реализован для локальных файлов
- [ ] Можно загрузить данные из CSV/Parquet файла
- [ ] Ресэмплинг работает корректно (1m → 5m, 1h, 1d)
- [ ] Валидация данных работает и детектит проблемы
- [ ] Хранилище сохраняет данные с правильной структурой
- [ ] Каталог датасетов работает (add, search, list)
- [ ] Версионирование вычисляет корректные хэши
- [ ] CLI команды работают
- [ ] Логирование всех операций

### Тесты (tests/unit/data/):
- [ ] test_schemas.py - валидация Pydantic моделей
- [ ] test_local_loader.py - загрузка из файлов
- [ ] test_resampler.py - корректность ресэмплинга
- [ ] test_validators.py - все виды валидации
- [ ] test_storage.py - сохранение/загрузка
- [ ] test_catalog.py - работа каталога
- [ ] test_versioning.py - хэширование и версионирование

### Интеграционные тесты (tests/integration/):
- [ ] test_data_pipeline.py - полный цикл загрузки и сохранения

### Производительность:
- [ ] Загрузка 1M баров < 5 секунд
- [ ] Ресэмплинг 1M баров < 10 секунд
- [ ] Валидация 1M баров < 3 секунды

## Промпты для реализации

### Промпт 1: Схемы данных
```
Реализуй Pydantic схемы для модуля данных в src/data/schemas.py:

1. OHLCVBar - схема для одного OHLCV бара:
   - timestamp: datetime с timezone
   - ticker: str
   - open, high, low, close: float (с валидацией соотношений)
   - volume: int (неотрицательный)
   - Кастомные валидаторы:
     * high >= max(open, close)
     * low <= min(open, close)
     * volume >= 0

2. DatasetMetadata - метаданные датасета:
   - dataset_id: UUID
   - ticker, timeframe: str
   - start_date, end_date: datetime
   - source: Literal['local', 'tinkoff', 'manual']
   - timezone: str
   - total_bars, missing_bars: int
   - hash: str (SHA256)
   - created_at: datetime
   - schema_version: str

3. DatasetConfig - конфигурация загрузки данных:
   - ticker: str | list[str]
   - timeframe: Literal['1m', '5m', '15m', '1h', '4h', '1d']
   - from_date, to_date: date
   - source_type: Literal['local', 'api']
   - file_path: Optional[Path]
   - resample_from: Optional[str]
   - timezone: str = 'UTC'
   - validate_data: bool = True

Используй Field для документации, валидаторы где нужно.
Следуй стандартам из docs/system/Development_Standards.md.
```

### Промпт 2: Загрузчики данных
```
Реализуй загрузчики данных в src/data/loaders/:

1. base.py - DataLoader Protocol:
   - load() метод с сигнатурой
   - get_available_tickers()
   - Docstrings с описанием контракта

2. local_file.py - LocalFileLoader:
   - Загрузка из Parquet и CSV
   - Автоопределение формата файла
   - Парсинг разных форматов дат
   - Валидация схемы
   - Конвертация timezone в UTC
   - Обработка ошибок (файл не найден, неверный формат)

3. cached.py - CachedDataLoader (decorator):
   - Обёртка над любым DataLoader
   - Кэш на диске (Parquet в artifacts/cache/)
   - LRU cache в памяти (lru_cache)
   - TTL для инвалидации
   - Логирование cache hits/misses

4. tinkoff_stub.py - TinkoffDataLoader:
   - Заглушка, raise NotImplementedError
   - Оставь TODO комментарии для будущей реализации

Используй type hints, логирование, обработку ошибок.
Протестируй на примерах CSV и Parquet файлов.
```

### Промпт 3: Хранилище и каталог
```
Реализуй систему хранения в src/data/storage/:

1. parquet_storage.py - ParquetStorage:
   - save_dataset(data, metadata) → DatasetMetadata
   - load_dataset(ticker, timeframe, from_date, to_date) → DataFrame
   - append_data(dataset_id, new_data)
   - list_datasets(ticker=None, timeframe=None) → list[DatasetMetadata]
   - delete_dataset(dataset_id)

   Структура на диске:
   artifacts/data/{ticker}/{timeframe}/{year}.parquet
   artifacts/data/{ticker}/{timeframe}/metadata.json

2. catalog.py - DatasetCatalog:
   - SQLite база в artifacts/db/catalog.db
   - Таблица datasets с полями из DatasetMetadata
   - Методы: add, search, filter, get_by_id
   - Индексы для быстрого поиска

3. versioning.py - DataVersioning:
   - compute_hash(dataframe) → str (SHA256)
   - compare_versions(hash1, hash2) → dict
   - get_history(dataset_id) → list[Version]
   - save_version_snapshot(dataset_id, data)

Обеспечь thread-safety для каталога.
Используй логирование всех операций.
```

### Промпт 4: Предобработка данных
```
Реализуй препроцессоры в src/data/preprocessors/:

1. timezone.py:
   - convert_to_utc(data: DataFrame) → DataFrame
   - validate_timezone_aware(data) → bool
   - localize_timestamp(ts, tz) → datetime
   - handle_dst_transition(data) - обработка перехода на летнее время

2. resampler.py - TimeframeResampler:
   - resample(data, from_tf, to_tf) → DataFrame
   - Агрегация OHLCV: open=first, high=max, low=min, close=last, volume=sum
   - Выравнивание по началу периода
   - Обработка неполных баров
   - Поддержка: 1m→5m, 1m→15m, 1m→1h, 1m→4h, 1m→1d

3. merger.py - DataMerger:
   - merge_sources(sources: list[DataFrame], strategy) → DataFrame
   - resolve_conflicts(data, priority_order)
   - fill_gaps(data, method='forward_fill' | 'interpolate')
   - validate_after_merge(data)

Все функции должны быть idempotent.
Добавь проверки корректности агрегации.
```

### Промпт 5: Валидация данных
```
Реализуй валидаторы в src/data/validators/:

1. integrity.py - IntegrityValidator:
   - check_duplicates(data) → ValidationResult
   - check_missing_bars(data, timeframe) → ValidationResult
   - check_monotonic_timestamps(data) → ValidationResult
   - check_completeness(data) → ValidationResult

2. schema.py - SchemaValidator:
   - validate_columns(data, required_columns) → ValidationResult
   - validate_dtypes(data) → ValidationResult
   - validate_ohlc_relationships(data) → ValidationResult
   - validate_timestamp_format(data) → ValidationResult

3. quality.py - QualityValidator:
   - detect_price_anomalies(data, threshold=3.0) → ValidationResult
   - check_volume_sanity(data) → ValidationResult
   - check_spread(data, max_spread_pct=10.0) → ValidationResult
   - generate_quality_report(data) → dict

ValidationResult должен содержать:
- is_valid: bool
- errors: list[str]
- warnings: list[str]
- statistics: dict

Используй статистические методы для детекции аномалий.
```

### Промпт 6: Пайплайн и CLI
```
Реализуй:

1. src/pipelines/data_preparation.py - DataPreparationPipeline:
   - __init__(config: DatasetConfig)
   - run() → DatasetMetadata
   - Этапы:
     a. Загрузка через loader
     b. Валидация всеми валидаторами
     c. Предобработка (timezone, resample если нужно)
     d. Сохранение в storage
     e. Регистрация в catalog
   - Логирование каждого этапа
   - Обработка ошибок с rollback
   - Возможность перезапуска с чекпоинтами

2. src/interfaces/cli/data_commands.py - CLI команды:
   - load_data() - загрузить данные
   - list_datasets() - показать список
   - validate_dataset() - проверить
   - resample_dataset() - ресэмплировать
   - dataset_info() - информация
   - export_dataset() - экспорт

Используй click для CLI.
Добавь progress bars (tqdm).
Красивый вывод в терминал (rich или colorama).
```

### Промпт 7: Тесты
```
Создай тесты для модуля данных:

tests/unit/data/:
- test_schemas.py - валидация Pydantic моделей, edge cases
- test_local_loader.py - загрузка разных форматов
- test_resampler.py - корректность ресэмплинга с известными данными
- test_validators.py - все валидаторы
- test_storage.py - save/load/append
- test_catalog.py - CRUD операции
- test_versioning.py - хэши и версии

tests/integration/data/:
- test_data_pipeline.py - полный цикл от файла до storage

Используй:
- pytest fixtures для тестовых данных
- parametrize для разных сценариев
- tmp_path для временных файлов
- mock для external dependencies

Покрытие должно быть > 80%.
```

## Важные замечания

### Форматы данных:
- **Основной формат**: Parquet (быстрый, эффективный)
- **Поддержка CSV**: Для совместимости
- **Encoding**: UTF-8
- **Timezone**: Всегда UTC в хранилище, конвертация при загрузке/выгрузке
- **Пример сырой строки CSV из getHistory**:
  `cbdf1d32-5758-490e-a2b1-780eaa79bdf7;2020-01-03T07:04:00Z;13.37;13.37;13.37;13.37;2;`

### Структура DataFrame:
Стандартные колонки (порядок важен):
```python
['timestamp', 'ticker', 'open', 'high', 'low', 'close', 'volume']
```
- timestamp: pd.DatetimeIndex с timezone
- ticker: str
- OHLC: float64
- volume: int64

### Оптимизация производительности:
- Используйте pandas vectorized операции
- Избегайте iterrows(), используйте apply() или numba
- Для больших файлов - chunked чтение
- Parquet с compression='snappy'

### Обработка ошибок:
- Валидационные ошибки → ValidationError с подробным описанием
- IO ошибки → DataLoadError
- Все ошибки логировать с контекстом
- Graceful degradation где возможно

### Интеграция с Tinkoff Invest API:
- Исторические данные загружаем через REST метод `getHistory`, доступный по адресу [https://tinkoff.github.io/investAPI/get_history/](https://tinkoff.github.io/investAPI/get_history/).
- Endpoint возвращает годовые архивы минутных свечей и ограничен 30 unary-запросами в минуту, поэтому необходим встроенный rate limiter, батчирование запросов и стратегия безопасных повторов.
- Необходимо кешировать загруженные архивы (локально и через `CachedDataLoader`), чтобы минимизировать количество обращений к API и оставаться в пределах лимитов.
- Выпуск и ротацию токенов доступа организуем по инструкции [https://tinkoff.github.io/investAPI/token/](https://tinkoff.github.io/investAPI/token/); токен передаём в каждом запросе и отслеживаем срок его жизни.
- Для уточнения контрактов и автогенерации клиентов используем официальный репозиторий [https://github.com/Tinkoff/investAPI](https://github.com/Tinkoff/investAPI).

### Контроль полноты загрузки:
- Перед скачиванием запрашиваем у API дату листинга/начала торгов для каждого тикера и формируем перечень обязательных годов.
- После загрузки сверяем наличие архивов по каждому году и тикеру (downloaded ↔ extracted ↔ processed). Любые пропуски фиксируем и повторяем загрузку автоматически.
- Конфигурация тикеров: допускается явный список в конфиге (`DatasetConfig.ticker`/`list[str]`) или путь к файлу `tickers_file`, содержащему перечень FIGI/тикеров для пакетной выгрузки.
- После обработки формируем отчёт о покрытии (процент скачанных файлов, количество повторных попыток, список отсутствующих лет) и сохраняем его в каталоге версионирования.
- При активном `backfill_missing` инициируем асинхронную докачку только недостающих архивов, повторяя цикл «скачивание → распаковка → конвертация → валидация» до полного покрытия периода от года листинга до текущего года.
- Флаг `update_latest_year` запускает периодический job, который догружает свежие архивы и обновляет только последний год, синхронизируя минутные данные и производные таймфреймы без пересоздания исторических файлов.

### Идемпотентность:
- Повторный запуск загрузки должен детектить существующие данные
- Использовать хэши для определения изменений
- Append только новых данных, не дублировать

### Версионирование:
- Schema version для обратной совместимости
- При изменении схемы - миграции

### Логирование:
```python
logger.info("Loading data", extra={
    'ticker': ticker,
    'from_date': from_date,
    'to_date': to_date,
    'timeframe': timeframe,
    'source': source_type
})
```

### Примеры использования:
```python
# Загрузка из файла
config = DatasetConfig(
    ticker='SBER',
    timeframe='1m',
    from_date=date(2020, 1, 1),
    to_date=date(2023, 12, 31),
    source_type='local',
    file_path=Path('data/sber.parquet')
)

pipeline = DataPreparationPipeline(config)
metadata = pipeline.run()

# Ресэмплинг
resampler = TimeframeResampler()
data_5m = resampler.resample(data_1m, '1m', '5m')

# Поиск в каталоге
catalog = DatasetCatalog()
datasets = catalog.search(ticker='SBER', timeframe='5m')
```

## Следующий этап
После завершения переходите к [Этапу 02: Обработка и валидация данных](Этап_02_Обработка_и_валидация_данных.md)
